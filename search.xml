<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[List 与 Stream 在 Spring Data MongoDB 场景下的比较]]></title>
    <url>%2Flist_vs_stream_in_spring_data_mongodb.html</url>
    <content type="text"><![CDATA[对比如果只是简单地用在接口返回，List 与 Stream 并没有差异例如以下两段代码，返回的结果是一样的 @GetMapping("list") public Object listAll() { return repository.findAll(); } @GetMapping("stream") public Object streamAll() { return repository.streamAllBy(); } 但如果你需要在业务逻辑中处理大量的数据，List 和 Stream 的差异就体现出来了。 如果不用 Stream，在处理海量数据的时候，为了避免一次性将全部数据 Load 到内存导致内存溢出，一般我们会进行分页处理。但 skip &amp; limit 跳页会导致较低的的查询性能，因此一般我们会采用 lastId 配合索引的方式来进行分页，如下所示 final int PAGE_SIZE = 5000; String lastId = '000000000000000000000000'; List&lt;MyDoc> rows = repo.findAllByGreaterThanId(lastId, PageRequest.of(0, PAGE_SIZE)); while (rows.size() > 0) { for (row in rows) { // do something for current row } // read next page lastId = lastOne(rows).getId(); rows = repo.findAllByGreaterThanId(lastId, PageRequest.of(0, PAGE_SIZE)); } 但这种方式仍然存在一些问题： 仍然会占用一些内存（取决于你设定的 PAGE SIZE），当然这不是什么大问题 在等待传输完当前页数据的 I/O 期间，应用程序什么也干不了 如果哪天要修改成多线程版本以提升处理效率，会有比较大的改动 代码复杂度稍高 相比之下，如果我们用 Stream 来处理，代码就简单多了 repo.streamAllBy().forEach(doc -> { // do something for cuurent row }); 如果要修改为并行版本也非常简单 repository.streamAllBy().parallel().forEach(doc -> { // do something for cuurent row }); Stream 的缺点： 批处理的场景下没有分页直观（例如滑动窗口），这点主要是 JDK8 缺乏支持，其它类似的框架如 RxJava 或 ProjectReactor 都是支持的，Spring Data Reactive 也有相关的支持（当然，学习成本也是很高。。） 【实际与我猜想的不一样，见实测章节】Stream 处理任务的期间会持续占用一个连接，不利于资源的复用。相比之下 List 只有每次拉取页的 I/O 期间才占用连接（假如不加事务的话）。如果连接资源很紧张，使用 Stream 可能会出较大的问题 性能实测环境： 单 collection 约 130w 数据 客户端：Java + MacOS List 可以看到，调用 List 的过程，JVM 内存只增不减，且 GC 频率越来越高。整个过程花了接近 15min 时间。 而在执行完后，触发一次 GC，直接内存占用就清零了。 原因显而易见，List 操作需要在 JVM 内存中构建 ArrayList 对象，加上数据量过于庞大，会导致不断地进行扩容，因此性能极差。同时由于所有数据均被一个 ArrayList 对象持有，导致内存占用只升不降（无法被 GC 回收） Stream handle count: 1391665. time elapsed: 11500ms 首先性能上远远高于 List（没有扩容和 GC，只花了 11s 左右） 由于不需要通过 ArrayList 去保存数据，内存利用率会迅速增加(约 700MB)，后面有一段维持直线的，猜测是因为一直没有触发 GC。 将代码稍微改动下，在 stream 的处理期间手动触发一些 GC repository.streamAllBy() .forEach(doc -> { String itemInMemory = doc.getContent(); if (c.get() % 100000 == 0) { System.gc(); } c.getAndIncrement(); }); handle count: 1391665. time elapsed: 15226ms 可以看到相比于 List，Stream 的处理期间是可以释放被占用的内存的。另外由于多了取余和 GC 的操作，整个时间花费也由 11s 上升到 15s，CPU 也有所上升。 缺点 2 实测为了验证上述的缺点 2，我准备了一个简单的服务以及两个接口 @GetMapping("list") public Object listAll() { return repository.findAll(); } @GetMapping("stream") public Object streamAll() { repository.streamAllBy().parallel().forEach(doc -> { try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } }); return "ok"; } 其中，/stream接口中针对每条数据会 sleep 200ms 的时间以模拟慢操作（数据库中预先准备了 100 条数据，因此该接口需要执行约 20s 的时间），/list接口则只是简单的返回所有数据。我们通过交叉用这两个接口来观察应用中的连接使用情况 我们先重启应用，确保连接池为空 先调用/list接口，观察日志会发现 Spring Data Mongo 创建了一个连接 2023-01-20 17:09:19.576 INFO 77750 --- [nio-8080-exec-1] org.mongodb.driver.connection : Opened connection [connectionId{localValue:2, serverValue:1859}] to 192.168.11.180:32017 如果此时进入断点查看，会发现连接池数量为 1继续调用/list接口（非并发场景）会发现 Mongo Client 将一直复用此连接，不会创建新的连接。这符合连接池的设计机制 我们先调用/stream接口，在其处理期间再调用/list接口，如果如我们所猜想的一样 Stream 会长时间占用一个连接的话，那么我们在调用/list接口的时候 Mongo Client 应该会再创建一个连接用于处理查询才对 实际情况是：在我们/stream接口执行期间，调用/list接口并没有使得 Mongo Client 创建新的连接。打断点观察ServerSessionPool的可用连接数也会发现其仍然为 1。显然 Mongo Client 及 JDK Stream 底层是针对这种情况做过优化的，猜想被推翻 为了证明在资源不够用的时候 Mongo Client 确实是会自动创建新的连接的，我们也用 ab 来做一个简单的压测 压测命令：ab -n 100 -c 5 &#39;localhost:8080/mongo/list&#39;可以看到控制台输出了 4 个连接创建的事件 2023-01-20 17:19:42.172 INFO 77750 --- [nio-8080-exec-3] org.mongodb.driver.connection : Opened connection [connectionId{localValue:6, serverValue:1865}] to 192.168.11.180:32017 2023-01-20 17:19:42.172 INFO 77750 --- [nio-8080-exec-4] org.mongodb.driver.connection : Opened connection [connectionId{localValue:5, serverValue:1863}] to 192.168.11.180:32017 2023-01-20 17:19:42.172 INFO 77750 --- [nio-8080-exec-2] org.mongodb.driver.connection : Opened connection [connectionId{localValue:4, serverValue:1864}] to 192.168.11.180:32017 2023-01-20 17:19:42.172 INFO 77750 --- [io-8080-exec-10] org.mongodb.driver.connection : Opened connection [connectionId{localValue:3, serverValue:1866}] to 192.168.11.180:32017 断点观察连接池可用连接数也变成了 5 结论除了如批处理之类的少数场景下，Stream 几乎总是优于分页 List（更不用说全量 List），因此在单次要处理的数据量达到一定量级时（比如超过 1000），应该优先考虑使用 Stream。 参考资料 https://stackoverflow.com/questions/63115831/spring-data-repository-list-vs-stream https://stackoverflow.com/questions/50698222/connection-pooling-in-spring-boot-and-mongo-db https://stackoverflow.com/questions/23808264/how-to-get-connected-clients-in-mongodb]]></content>
      <categories>
        <category>middleware</category>
        <category>mongodb</category>
        <category>client</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用 Wireshark 抓包 Redis]]></title>
    <url>%2Fredis-wireshark.html</url>
    <content type="text"><![CDATA[Redis 有自己的一套传输协议（RESP），因此想通过 Wireshark 抓包 Redis 得先安装插件，具体步骤如下： 找到你的 Wireshark 安装目录下的 init.lua 所在的文件夹 例如我的是 MacOS，通过 DMG 安装的，是在 /Applications/Wireshark.app/Contents/Resources/share/wireshark 目录下 把 redis-wireshark.conf 整个文件 copy 放在上述目录下 编辑 init.lua 添加以下内容，加载第 2 步放进去的 lua 脚本 -- 其它内容... if not running_superuser or run_user_scripts_when_superuser then dofile(DATA_DIR.."console.lua") dofile(DATA_DIR.."redis-wireshark.lua") -- 这行是你要新加的，其它都是原有的 end -- 其它内容... 重启 Wireshark 或使用 UI 上的 Analyse -&gt; Reload Lua Plugins 功能（CMD + SHIFT + L）重新加载插件 启动抓包，用 redis-cli 连接你的 Redis，随便输入一些命令（我这里示例是 get tac），在 Wireshark filter 框框中输入 redis 或 tcp.port == 6379 过滤协议，看到类似以下的数据就说明成功了]]></content>
      <categories>
        <category>middlewire</category>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[只靠一个米家门窗传感器搞定浴室自动化]]></title>
    <url>%2Fbathroom_automation.html</url>
    <content type="text"><![CDATA[背景最近心血来潮买了个米家门窗传感器，打算用在浴室实现一种人来灯亮，人走灯灭的效果，但买回来后却发现实际场景比我想象中要复杂得多。为啥呢？门窗传感器最大的问题在于只能感知开/合两种状态，所以无法得知是否有人在浴室内。如果你仅靠闭门来触发关灯的话，就会有一个大问题：当你触发开门的时候灯亮了，然后你走进浴室，这时关门，pa 的一下，灯灭了。。尴尬。。 方案如果要解决这个问题，首先想到的是再买个人体红外感应器配合来用。但一来吧，总觉得为了这一个小自动化功能买两个传感器不经济；二来红外感应器在浴室的表现并不好（覆盖角度有限制而且有些浴室设计还是分区的，加上浴室水蒸气有时会干扰红外检）因此不是个很好的方案。 但东西都买回来了，总不能丢一边吃灰吧。好在办法总比困难多。门窗传感器本身虽然不足以检测到人，但米家给我们提供了其它逻辑工具啊~ 来看看这个问题，其实可以抽象成如何设计一个有限状态机（Finite Automate）以保证浴室自动化符合我们的要求的问题，其中： 要处理的浴室状态有： close_out: 室内无人且门属于关闭状态（此状态时灯与排气扇关闭） open_wait: 室内无人且门属于打开等待进入状态（此状态时灯与排气扇打开） close_in: 室内有人且门属于关闭状态（此状态时灯与排气扇保持打开） open_in: 室内有人且门属于打开状态（此状态时灯与排气扇区保持打开） tips: 因为还要考虑人是否在内，所以要考虑的状态有四个而非简单的开和闭两个。 影响状态变换的条件有： open: 浴室门打开 close: 浴室门关闭 但实际上还有个隐藏条件可以利用： time_elaspe: 时间流逝 ns（比如我们假定现实中 90% 的场景下人进入浴室，都会在 15s 内关上门） 如果把人工干预也算上，那还可以加上： manual: 检测到手动操作（例如，手动关灯） 最终设计出来的状态变换图如下 其中 close_out 即是起始状态也是最终状态。 对应状态转换表如下 open close time elaspe manual close_out open_wait - - - open_wait - close_out open_in - close_in close_out - - close_out open_in - close_out - - 横杠 - 表示该状态下不接受此输入的意思 验证接下来我们把要处理的场景跑一遍，看上述 FA 能否满足我们的要求。 为了简化表述，我们把状态和条件均用数字代替： state set: close_out(1), open_wait(2), close_in(3), open_in(4) criteria: open(1), close(2), time_elaspe(3) 场景一：长时间事务，会关门进行（如洗澡，蹲坑）输入：1, 2, 1, 2状态变换过程：1-&gt;2-&gt;3-&gt;4-&gt;1 最终状态为 close_out，可接受，符合预期 子场景 1.1：人在里边临时开门（不超时）输入：1, 2, 1, 2状态变换过程：1-&gt;2-&gt;3-&gt;4-&gt;1最终状态为为 close_out，可接受，但不符合预期（预期状态是保持 close_in），需要将输入调整为 1, 2, 1, 2, 1, 2（也就是临时开门后再执行一次开关门，中间会经历一次灯断电，体验稍差）状态变换过程：1-&gt;2-&gt;3-&gt;4-&gt;1-&gt;2-&gt;3，符合预期 场景二：短时间事务，不关门（如洗手，时间足以触发条件 3）输入：1, 3, 2状态变换过程：1-&gt;2-&gt;4-&gt;1 最终状态为 close_out，可接受，符合预期 场景三：误操作，开门后立马关门（时间不足以触发条件 3）输入：1, 2状态变换过程：1-&gt;2-&gt;3最终状态为 close_in，非可接受状态，因此要人工介入，使得 输入变为： 1, 2, 4状态变换过程：1-&gt;2-&gt;3-&gt;1 最终状态回到 close_out，可接受，符合预期 总结可以看到在场景 1.1 &amp; 3 下若没有人工介入依然是无法很好地处理的。好在 1&amp;2 占了我们日常场景的 90% 以上，总的来说此方案还是利大于弊。 实现由于笔者使用的是米家 APP，因此这里仅展示基于米家『场景』模块的实现（理论上只要支持上述触发条件的平台都能实现） 这里的场景指的是米家 APP 上的『场景』模块，与我们文章上述提到的场景并非同一概念米家场景分为自动（又称作智能）和手动两类，手类执行的场景除了手动操作外，还可以通过自动场景来触发 我们把浴室的状态与米家的场景对应起来（有些状态可能会对应多个场景），得到以下场景 close_out 触发条件：门窗传感器开 执行动作： 开浴室灯 &amp; 排气扇（这块可以根据实际需求自行调整） 关闭智能 close_out 开启智能 open_wait 关闭智能 open_in 执行手动场景 open_wait.time_elaspe tips: 其中命名 close_out 代表该米家场景打开时我们的浴室是处在状态 close_out 的，一旦检测到门窗传感器打开将执行相应动作。动作 2&amp;3 组合起来实现了状态切换的效果（关闭当前智能，打开目标状态对应的智能，也就是 open_in）；动作 5 是模拟以时间条件为输入的效果，详见 open_wait.time_elaspe 的配置此外，这里还出现了一个看似没啥用的动作 关闭 open_in，将在后面作解答 open_wait.time_elaspe 触发条件：手动触发 执行动作： 延时 15s（时间可以自行调整成符合家庭习惯的数值） 开启 open_in open_wait 触发条件：门窗传感器关 执行动作： 关闭智能 open_wait 开启智能 close_in 开启智能 close_in.manual 关闭智能 open_in tips: 这里的状态切换是由动作 1&amp;2&amp;3 共同组成的，这是因为在我们的状态转换表里面 close_in 状态是可以被条件 open / manual 任其一触发转换到其它状态，所以状态 close_in 其实是对应了两个米家智能，这两个智能只能是同时开启或同时关闭，才能保证自动化不会出现混乱 close_in 触发条件：门窗传感器开 执行动作： 关闭智能 close_in 关闭智能 close_in.manual 开启智能 open_in close_in.manual 触发条件：浴室灯 or 排气扇任一关闭（这块可以根据实际需求自行调整） 执行动作： 关浴室灯 &amp; 排气扇（这块可以根据实际需求自行调整） 关闭智能 close_in 关闭智能 close_in.manual 开启智能 close_out 关闭智能 open_in open_in 触发条件：门窗传感器关 执行动作： 关浴室灯 &amp; 排气扇（这块可以根据实际需求自行调整） 关闭智能 open_in 开启智能 close_out 最后，还记得上面提到的 关闭 open_in 这个看似没啥用的动作吗，其实不仅在 close_out 状态中，在除 open_in 外的每一个状态动作执行时都要加上这个动作。这是因为利用米家 APP 的延时模拟时间条件的输入这个是无法取消的，即是说一旦执行了 open_wait.time_elaspe，15s 后必定会打开 open_in 场景，而这时我们的浴室可能处在任何一个状态。为了避免因此导致的混乱，我们才需要在每个状态的动作加入此动作 tips: 超时效果也可以通过门窗传感器自带的超时未关通知能力来实现，这样就相当于是可中断的，不需要上述的的补偿操作了。但有个问题是，目前我这款传感器好像只支持配置分钟时间粒度 手机截图如下 结语以上配置对于普通用户（尤其是没有编程基础的）来说还是有些过于复杂了，建议厂家可以将状态机直接集成到门窗传感器中的，用户只要决定用或不用即可。 另外，米家平台提供的基础能力也比较有限（可能是基于易用性及安全性的考虑？），导致很多效果实现起来还是比较困难。不管怎样，希望希望米家能加入一些变量能力，可以通过动作来设置变量值，也可以根据变量值的不同来触发不同的智能，这样会有更高的可玩性。]]></content>
      <categories>
        <category>home automation</category>
      </categories>
      <tags>
        <tag>智能家居</tag>
        <tag>米家</tag>
        <tag>有限状态机</tag>
        <tag>DFA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次 springfox 扩展之旅 —— 动态 api 文档]]></title>
    <url>%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%20springfox%20%E6%89%A9%E5%B1%95%E4%B9%8B%E6%97%85%20%E2%80%94%E2%80%94%20%E5%8A%A8%E6%80%81%20api%20%E6%96%87%E6%A1%A3.html</url>
    <content type="text"><![CDATA[出于 Pigeon 项目的需要，我需要把通过插件支持的扩展信息在 swagger api 文档中动态展示出来。 项目使用的是 springfox 框架生成的 swagger 文档，众所周知 springfox 是注解型的文档框架，而且文档内容是直接写死的，不支持动态化，估摸着是需要自己扩展了。 按照惯例，先了解代码架构（由于以前都是使用为主，没有深入了解，也不知道有哪些拓展点）。发现 springfox 其实是在应用启动时 scan 带有 swagger 注解的类和字段，生成一个叫 Documentation 的类，然后通过 mapper 转换成相应的 swagger model（v1.2 or v2）。 可见，我们要分析的 code 可以缩小到 Docuementation 生成的过程，主要关注 @ApiParam, @ApiPropertyModel 这几个注解的解析步骤。 继续深入，发现 Documentation 的生成是通过 DocumentationPlugin 来做的，而调度 plugins 则是在 DocumentationPluginsBootstrapper 中。DocumentationPluginsBootstrapper 又是从哪里被调用的呢？没错，是通过 SpringfoxWebMvcConfiguration scan package springfox.documentation.spring.web.plugins 时被加载到 spring context 中的，由于这是一个 SmartLifecycle，因此会自动执行 #start 方法。在这个方法中会 foreach plugins，通过 plugin 得到的文档 context（context 中包含了 api 信息，而这些信息其实正是 spring mvc 维护着的 request handlers），再经由 scanner scan 整个 context 生成 Documentation 后加入到 DocumentationCache 中，后续通过 /api-v2/docs 接口访问时就是直接从 cache 中去获取到 Documentation 的。 上面提到的 plugins 又是什么呢？这个概念大家可能比较陌生，但只要是用过 springfox 的小伙伴，对 Docket 这个类肯定不陌生。没错，Docket 其实也是实现了 DocumentationPlugin 的一个类，根据官方的描述 Docket is a builder which is intended to be the primary interface into the Springfox framework.Provides sensible defaults and convenience methods for configuration. 回归主题，DocumentationPluginsBootstrapper 使用的 scanner 是 ApiDocumentationScanner。追踪源码发现，这个 scanner 中又细分为 ApiListingReferenceScanner（扫描引用 model 的）ApiListingScanner （扫描 apis 的），后者正是关键类。继续深入，在 ApiListingScanner -&gt; ApiDescriptionReader -&gt; ApiOperationReader 中发现，Operation（即维护 api 的路径、参数等具体描述的类）正是在其中通过 20+ OperationBuilderPlugin 组成的 filter 器链共同构建而成。根据名称大概筛选了一下，最终锁定了 OperationParameterReader 这个类。该类在处理参数时又会分成需要 expand 的（@RequestBody 之流）和不需要 expand 的（@ApiParam 直接标注的参数）。 先来看不需要 expand 的，最终会交由 ParameterBuilderPlugin 链处理，因此我简单写了一个扩展 @Component public class PigeonExtendParamBuilder implements ParameterBuilderPlugin { @Override public void apply(ParameterContext context) { if (context.resolvedMethodParameter().getParameterType().isInstanceOf(String.class)) { context.parameterBuilder() .allowableValues(new AllowableListValues(Lists.newArrayList("MAIL", "SMS"), "String")); } } @Override public boolean supports(DocumentationType documentationType) { return true; } } 跑下程序，果然所有 @ApiParam 标注的 string 类型参数都被加上了 Available values : MAIL, SMS 限制。但在 body 定义里面的参数没有效果，一开始推测是跟上述的 expand 概念有关，但后来发现不是，而是这类参数最终会指向一个 ModelRef，因此应该是与 Model 的解析有关。那 Model 的解析是在什么时候呢，同样是在 ApiListingScanner#scan 方法中，交由 ApiModelReader 完成。 ApiModelReader 会通过由 Spring MVC 维护的 RequestMapping 信息提取 model 信息（例如 @RequestBody, @Requestpart 之流），然后交给 ModelProvied#modelFor 转换成 springfox 的 Model 对象，此 Model 对象即是 ModelRef 引用的 Model。 在 #modelFor 中，会通过反射解析出 Model 对应的 class 中的所有字段信息（称为 ModelProperty），而这个过程则是交给 ModelPropertiesProvider#propertiesFor 完成的，其中细节比较多，但总的来说，最终都是交由 ModelPropertyBuilderPlugin 链来处理的，这个类也正是我们要找的扩展点。后续简单验证了一下，确实是有效的。 最终效果：TODO]]></content>
  </entry>
  <entry>
    <title><![CDATA[有关沟通成本的一点点思考]]></title>
    <url>%2Fthinking_about_cost_of_communication.html</url>
    <content type="text"><![CDATA[团队协作最大的成本是沟通成本。如何降低沟通成本，是管理者需要持续思考的问题。 日常沟通的本质，究其根本其实是由一系列的逻辑推理构成。而所谓的达成一致意见，即是我们对同一个命题推导出了同样的结论。 对于这一点，可能有些同学并不认同。这是因为我们的交流往往不会以十分正式的逻辑推理过程进行，而是更多的口语化进行。 比方说：今天应该会下雨。这样简单的一句话就包括了逻辑推理，只是大前提小前提均没有明确而以，需要我们根据当时聊天的语境来确定。假如你是因为昨天看了天气预报，那你的大前提是：按以往经验，如果天气预报预测说会下雨，那大概率会下雨；小前提是：昨天的天气预报说今天会下雨；结论就是：今天应该会下雨。如果换一种场景，是因为你看到今天的天空，那你的大前提是：阴云密布的天气大概念会下雨；小前提是：今天是阴天；结论就是：今天应该会下雨。 可见，简单的一句话，也是包含了完整的逻辑推理在内的。只不过出于方便，我们在交流时往往会简化许多步骤，并通过我们的大脑运算去补全其它的信息。否则，我们在沟通时的对话便会冗长而啰嗦，有效信息比低，因而沟通成本高。 但这样做也有缺点，就是我们有可能意会错别人的意思，这时就出现了所谓的误解。 假设甲对乙说今天应该会下雨，实际上是因为甲昨天看了天气预报。但乙并没有看，而且乙今天出门时看到天气晴朗，因此并不同意甲的说法。此时要保证沟通顺畅，需要甲补充自己的大前提。如果甲也并没有意识到这个问题，不去向乙解析清楚，那么两个人之间就产生了分岐，产生的原因是沟通中信息的缺失。 除了信息缺失外，信息畸变（想要传递的信息，因为表述不当，导致别人理解错误）也会带来误解，这可能是有意的（偷换概念），也可能是无意的（混淆概念），但导致的结果一样。像同样是偷，偷窃、偷笑就完全不是同一个概念。 上面讨论的是在信息传递本身，但如果沟通本身缺少主题（即没有要讨论的命题），自然也谈不上得出结论，就是在白白浪费时间而以。 知道了原因，我们便可以确定下来沟通中应尽力遵守的一些原则： 讨论前确定命题（有时比较迷茫的时候，想借讨论过程本身找到问题其实也是一个命题） 尽可能使用术语（术语即是某个领域中的专用概念，保证双方都理解术语的前提下，使用术语可以很有效地降低沟通成本） 尽可能复用现有术语（少用一些不常见或自创的概念） 有意识地创造一些概念，但需要做到 尽力保证其定义准确 做好概念普及工作，让团队能在一个上下文沟通 一经确定便不要再修改，这容易带来混乱 避免大量创造不准确、难以理解的概念（这点在编码中的类、方法、变量命名尤为常见） 用人过程中常见的雷区专业基础太差专业基础太差的另一种解释是：对这个专业领域的概念理解得太少。 这会导致你在解释一些问题给他听的时候，需要费非常大的工夫。因为逻辑推理是一层嵌一层的结果，一个上层概念往往是由无数下层概念支撑起来的，如果他对下层概念的理解不够，他是很难去理解一个上层概念的。 我听到很多朋友问过我一个问题：什么是 Java？内行人士可能完全能理解什么是 Java，但是如果让你向外行解释清楚，你会发现这其实是很困难的一件事。 这是为什么？因为我们要真正理解一个概念，不仅要知道其定义，更要理解其内涵。 Java 是一个上层概念，它是一门静态的、面向对象的编程语言。但如果你照搬此定义向你的朋友解释，他听完肯定依然一脸懵圈。要对这个概念有深刻的理解，你首先要理解更下一层的概念：语法、框架、虚拟机、IDE 等等。而要理解下一层的概念又要理解更下一层的概念，比如： 语法的下一层概念有：修饰符、关键字、变量、常量、类、方法等等 虚拟机的下一层概念有：字节码、堆、栈、class 文件、垃圾回收、操作系统、等等 如此循环往复，每个上层概念都会牵扯出一堆组成树状结构的概念，在不理解底层概念的情况下试图直接去理解上层概念，那必然会非常困难而且理解得也不深刻。 这些下层的概念其实都已经内化在内行人士心中了，所以跟他们交流时无需过多地解释。但对于外行则完全不一样，这就导致很多时候我们面对这种问题非常无奈。 理解能力太差专业基础差还有一种情况，就是许多概念都知道一点，但不深入，或者理解有错误。这种情况其实比上面更可怕，不懂的人会直接表现出来，而理解有偏差的人，不仅不会表现出来，他们往往还会表现得自己是正确的一样，进而把不明就理的人带歪。 这也是为什么有人主张在面试的时候，遇到不懂的问题，直接说不懂也好过硬答。 学习能力太差这是个信息爆炸的时代，每天都有新的知识产生、旧的知识消失，学习能力差的人终究会逐渐无法跟上时代的脚步。 但这个问题会在相对而言的中长期才体现出来，应该视团队当前情况决定是否要与此类人共事。 三观不合所谓三观不合，其实是在一些基础认知上存在分岐。比如甲是一个追求完美的人，而乙是一个得过且过的人，那他们在同一个问题上就很可能出现意见不合。而所谓江山易改、本性难移，基础认知是没那么容易改变的。多数人在某一个团队工作短则几月长也就几年，这点时间你是很难去改变一个人的。 最好的方法是不要与三观不合的人一起共事，这并不一定因为他的认知是错的，只是你们不适合。]]></content>
      <categories>
        <category>个人思考</category>
        <category>软件工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DDD 战术设计之 Java 服务端落地简要方案探讨]]></title>
    <url>%2Fddd_practice_in_java_server_side.html</url>
    <content type="text"><![CDATA[前言DDD 全称领域驱动设计，最初由 Eric Evans 提出。此文章仅探讨 DDD 在 Java 服务端的落地可行性简要方案，以期不需要太过复杂的前期准备也能够快速在项目中运用 DDD 编写代码 注意，此文章： 假定读者有一定的 DDD 理论基础 假定读者有 Java 服务端开发经验，并掌握主流的开发框架（如 Spring），本文的许多章节将会结合这些框架进行实现 以经典的 RBAC 权限模型为示例进行 DDD 建模 DDD 编写的代码所属层次我们把 DDD 设计的相关代码放到 Domain 层，这一层是介于经典三层架构中 Service 与 DAO 层之间的特殊的一层，但严格意义上来说还是属于 Service 层（处理业务逻辑），可以想象成在原先的 Service 层上又划分了一层出来。 如下图所示 TODO:: 示例下面是我们在 JAVA 工程中采用的一个 DDD 包结构规范 TODO:: 如果是现有项目，可以在与 controller, service 平级的 package 中再创建一个 domain，这样做的好处是不需要改动现有代码，domain 与 service 可以共存 TODO:: 也可以设计得更复杂，但成本较高，一般建议在新的、核心的项目中使用 TODO:: 核心概念的落地实体 以标识作为其基本定义的对象称为实体 - Eric Evans 实体至少有两个字段：唯一标识 id 和 负责该实体持久化工作的 DAO。 服务端比较流行 ORM 框架（如 MyBatis），这些框架操作的数据对象（DO）往往是一些纯数据模型，不适合将实体与 DO 混用，最好是以实体依赖 DO 的方式设计，将 DO 定位为存储实体属性的承载者 最终得到一个最小的实体定义如下： public class User { private Long id; private UserDAO dao; public User(Long id, UserDAO dao) { this.id = id; this.dao = dao; } public UserDO data() { return this.dao.selectById(this.id); } public String sayhello() { return this.data().getName() + " say: hello."; } } 使用时的代码如下 User user = new User(1L); user.data(); user.sayhello(); 实际项目中，实体往往还有很多其它依赖，而且随着业务的发展依赖还会不断增多。依赖多了以后，实体的构造就成了一个大问题，不可能再以 new 的方式去创建实体。因此，需要引入工厂的概念，这将在下文讨论 实体与数据对象的关系初学者的经常存在一个误区是，喜欢把实体与数据对象（或者说数据库表设计）一一对应起来。这种观念一定要纠正过来，在这里给出的建议是：实体的设计要更多从业务概念的角度去考虑（比如从技术的角度可能会细分出 user_info, user_ext, user_login_history 等等数据对象，但从业务的角度其实都应该是对应到用户这一个实体）。 常见的有以下情况 一个实体的属性拆分为多张表存储 实体之间的关联关系信息 public class UserDAO { private UserInfoMapper userInfoMapper; private UserExtMapper userExtMapper; private UserLoginHistoryMapper userLoginHistoryMapper; public UserDO selectById(Long id) { build( userInfoMapper.selectById(id), userExtMapper.selectById(id), userLoginHistoryMapper.selectById(id), ) } } 多封装一层 DAO 可以屏蔽持久化的底层实现，方便后续替换（比如某天要对现有服务进行拆分，只需将 mapper 替换成 feign client 即可），但同时也会增加工作量。 一种更加简便的方式是直接在实体中引用 mapper 进行操作。 public class User { private Long id; private UserInfoMapper userInfoMapper; private UserExtMapper userExtMapper; private UserLoginHistoryMapper userLoginHistoryMapper; public UserDO data() { ... } } 此外，出于高内聚考虑，实体不应该直接操作不属于自己管辖的数据对象。如，User 不应该通过 RoleMapper 直接操作 RoleDO 引用考虑到在服务端开发中，有状态的对象朝生夕灭的情况非常常见（服务端要管理的对象非常多，不可能将所有实体都存在内存中，一般一个请求过来时会创建对象，请求结束后在下一次 GC 这个对象就会被销毁），而实体之间的关联可能是非常复杂的，每次使用时都构建一个完整的聚合非常不划算，比较建议实体间的聚合采用软关联的方式 可以看到以下两种方式的区别： 硬关联public class User { private Long id; private List&lt;Role> roles; public User(Long id, List&lt;Role> roles) { this.id = id; this.roles = roles; } public List&lt;Role> listAllRoles() { return this.roles; } } 软关联public class User { private Long id; private RoleRepository roleRepo; public User(Long id, RoleRepository roleRepo) { this.id = id; this.roleRepo = roleRepo; } public List&lt;Role> listAllRoles() { return this.roleRepo.listAllByUserId(this.getId()); } } 两者在使用方式并没有区别 for (Role role : user.listAllRoles()) { role.dosomething() } 工厂虽然在上面我们采用了软关联的方式建立实体之间的引用关系，但这并不代表要构建一个实体就非常简单了，原因是我们的实体除了依赖其它实体外，往往还需要依赖许多其它对象（如领域服务、仓储、DAO 等），并且随着业务的变化，实体的依赖往往还会随之发生变化，如果还是通过传统的 new 方式去创建一个实体，会产生一些灾难性的问题： 使用者必须清楚实体的创建细节，这会大大增加代码的复杂度 每当实体的构造方式发生变化时，不得不调整所有创建实体的代码逻辑以解决代码编译问题 这个时候就需要引入工厂（Factory）的概念了，一个通用 Factory 的实现示例如下 @Component public abstract class Factory { @Autowired private static UserRepository userRepo; public User getUser(Long id) { return new User(id, userRepo); } } 结合 Spring，可以把实体的依赖注入做得更简单，并且在实体 User 的依赖变化后不需要做任何代码变更 public abstract class Factory { public User getUser(Long id) { User user = new User(id); SpringUtils.inject(user); // 结合 AOP 可以进一步简化装配逻辑 return user; } } 实体仓储（Repository） TODO::仓储可以为使用者提供实体的创建、删除及条件查询操作。在 C/S 中，仓储还要负责内存中的实体的更新（保证数据一致性）及缓存管理（防止频繁地重复创建，影响性能） 在 B/S 架构中，我们将实体与数据对象分离，因此数据一致性问题交给 ORM 去控制即可，仓储 仓储往往依赖 DAO（查询持久化数据）及工厂（创建实体），并且应可以发布领域事件。 领域服务领域服务用于处理一些在概念上不属于实体的操作，这些操作本质上往往是一些活动或行为，并且是无状态的。对于这类操作，将其强制进行归类会显得非常别扭，于是便引入了领域服务这一概念。 需要明确的是，其与三层架构的 Service 层（业务逻辑层）并不是一个概念。另外与 Evans 在书中提及的示例不同，为了避免混乱，一般不建议为领域服务的类命名加上 Service 后缀。可以简单理解为，领域中没有任何实体适合承载该职责，因此我们创造了一个『实体』来承担，只不过这个特殊的『实体』是一个无状态的单例而以。 示例在某个应用中，由于用户量较大，用户登录历史可能会逐渐变得非常庞大，因此需要需要定时查找超过 15 天的记录并将其清理。 显然，我们需要完成的这个操作无法归类到任何一个实体中，因此我们需要一个名为 LoginHistoryClearer 的领域服务来承接此职责 @Compoment public class LoginHistoryClearer { private UserRepository userRepo; public void clearOutDated(Integer interval) { for (User user : userRepo.listAll()) { user.removeOutDatedLoginHistory(interval); } } } 在其它地方，我们可以直接注入该领域服务，并使用 @Slf4j @Component public class UserScheduledTask { @Autowired private LoginHistoryClearer clearer; @Value("${exec.output.interval.days:15}") private Integer intervalDays; @Scheduled(cron = "0 0 0 * * ?") public void deleteExecData() { log.info("starting clear out dated data, intervalDays=>{}", intervalDays); clearer.clearOutDated(intervalDays); log.info("clear out dated data end"); } } 领域事件在我们的领域活动（实体、仓储等操作）中会出现一系列的重要的事件，而这些事件的订阅者，往往需要对这些事件作出响应（例如，新增用户后，可能会触发一系列动作：发送欢迎信息、发放优惠券等等）。领域事件可以简单地理解为是发布订阅模式在 DDD 中的一种运用。 在我们的实践中，一般采用事件总线来快速地发布一个领域事件。 事件总线的接口定义一般如下 public interface EventBus { void post(Event event); } 通过调用 EventBus.post() 方法，我们可以快速发布一个事件。 同时我们还会提供一个抽象类 AbstractEventPublisher public class AbstractEventPublisher implements EventPublisher { private EventBus eventBus; public void setEventBus(EventBus eventBus) { this.eventBus = eventBus; } @Override public void publish(Event event) { if (eventBus != null) { eventBus.post(event); } else { log.warn("event bus is null. event " + event.getClass() + " will not be published!"); } } } public interface EventPublisher { void publish(Event event); } 这样我们可以让实体或 Manager 继承自 AbstractEventPublisher，其便有了发布事件的能力。至于如何订阅并处理这些事件，取决于 EventBus 的实现方式。举个例子，我们一般使用 Guava 的 EventBus，定义相关的 handler 并注册到 EventBus 中便可方便地处理这些事件 @Component public class DomainEventBus extends EventBus implements InitializingBean { @Autowired private FooEventHandler fooEventHandler; @Override public void afterPropertiesSet() { this.register(fooEventHandler); } } @Component @Slf4j public class FooEventHandler implements DomainEventHandler { @Override @Subscribe public void listen(ProjectCreatEvent e) { // do something here... } } 其它TODO::● ddd-support DDD 设计理解了 DDD 中的全部概念，也并不意味着就能做出一个好的设计了。 DDD 的设计最重要的是做好以下几点： 准确地定义实体 准确地定义实体应该有哪些方法 确立实体与实体之间的关系 实体的设计其实是一个建模的过程。面向对象的设计方法本质就是将现实世界的对象关系以简化的形式提炼为模型。关于这一块，已经是一个更大的话题了，不在这里讨论。 案例工程TODO:: 其它与现有三层架构是否冲突并不冲突， 甚至是可以混用的 事务问题如何解决实体操作是可以兼容 JDBC 事务，在编排实体的应用层中加上 Spring 事务注解 @Transaction 即可 重复查询的性能问题如下，会执行三次数据库查询 log.debug(user.data()); log.info(user.data()); log.warn(user.data()); 实际项目中，应使用带缓存的 ORM 框架（如 MyBatis），这样便可避免同一事务中重复查询带来的性能问题。否则应注意优化编码方式，如下 UserDO data = user.data(); log.debug(data); log.info(data); log.warn(data); 复杂查询场景下的性能问题DDD 要求将数据对象转换为内存中的实体对象后再进行业务操作，这注定了 DDD 不擅长批量查询以及联表查询等复杂的查询场景。业界采用的方案是 CQRS 模式，将查询操作从领域模型中分离出去。 要实现也很简单，有查询业务时，直接定义一个相应的 Service，在里面操作数据库完成查询即可。 更复杂一点，可以专门为查询操作开一个微服务，实现物理上的分离。 示例如下 不分离，会存在对 dto, vo 等对象的直接引用，导致领域模型被污染 public class User { public List&lt;UserLoginHistoryVO> queryLoginHistory(QueryDTO dto) { return dao.queryLoginHistory(...) } } 分离后，查询操作转移到专门的查询 Service 中 public class UserQueryService { public List&lt;UserLoginHistoryVO> queryLoginHistory(QueryDTO dto) { return dao.queryLoginHistory(...) } }]]></content>
  </entry>
  <entry>
    <title><![CDATA[程序猿加班真的是常态吗? —— 浅谈如何治理一个复杂软件系统]]></title>
    <url>%2Fhow_to_manage_a_complex_software_system.html</url>
    <content type="text"><![CDATA[经常听到一些程序猿加班是常态的言论，在这里我想尝试下进行反驳。 先来分析一下这个命题： 程序猿加班是常态 =&gt; 程序猿的工作量巨大 =&gt; 软件系统过于复杂，难以处理。 可见，反驳这个命题的关键就在于回答『如何治理一个复杂的软件系统』这个问题。 接下来深入看下这个问题。软件系统是什么？我认为软件系统无非是一个巨大的逻辑体。你的任何操作（如点击 web 的一个按钮），其实背后都是一层又一层的逻辑堆砌的结果。 以浏览器渲染一个按钮（&lt;button&gt;click&lt;/button&gt;）为例，这其实是浏览器解析 HTML 代码的结果。HTML 本身是有逻辑可循的，你必须要按照他的规则编写才能正确地展示 UI，而 HTML 本身又需要浏览器进行解析，这个过程需要用到许多更低一层的逻辑（比如语法解释器，比如图形渲染的接口），这些被依赖的逻辑本身又会依赖更底层的逻辑（比如显卡接口）。 这些逻辑一层嵌一层的，直至最底层的 CPU 指令集。 因此任何一个复杂系统，本质上其实就是一个庞大的逻辑体。而这个逻辑体，必然是分层的（你见过谁直接用 GPU 的接口来渲染网站吗），否则描述它的复杂度就是指数级增长了，以人脑的计算能力很快就无法处理这么复杂的系统了（这就好比在概率论里面，让你连续抛掷10次硬币，再用文字表示其样本空间，你要是用枚举法来表示，可能得写到手软，但要是用笛卡尔积来表示，只需要一行即可）。 整个网站开发，其实也是分层的，其中最经典的莫过于三层架构了（视图层、业务逻辑层、持久层），演化到今天，视图层独立出去成了前端领域，业务逻辑与持久层则归为后端领域（当然，还有更底层的东西，比如说数据库，操作系统，只是大多数开发仔都不需要参与这一块的开发）。 看起来很完美，前人已经把分层都设计好了，并且那些高难度的底层开发工作往往也有开源组织在承担，稳定性都是经过验证的。我们做应用开发的，只要搞定视图层（前端）跟业务逻辑层（后端）不就好了吗？这点事情都搞不定吗？问题究竟出在哪呢？ 其一是，单独某一层内的复杂度，其实很多时候是超乎我们想象的，因此需要进一步分层。 以业务逻辑层为例，一个简单的用户注册，可能就包括了验证码校验，密码复杂度检测，手机绑定，注册成功欢迎邮件发送，发放优惠券等等业务逻辑，以及缓存、冗余之类的非业务逻辑。这些被依赖的逻辑其实也是同属于业务逻辑层的，并且也会被其他业务逻辑依赖，一旦没有做好划分，那又是重复造轮子，复杂度和工作量都会指数级增长！关于这一块，不同领域的解决方案不同，前端是组件化，而后端是面向对象（虽然是很标准的答案，但我认为真正掌握这两技能的研发同学其实很少，最典型的莫过于拿着面向对象语言，写着面向过程代码），非要抽象出一个更通用的方案的话，我认为可以称之为『建模』。前端通过建模，将视图层进一步分拆出组件层；后端通过建模，将业务逻辑层进一步分拆出领域模型层（在微服务架构中亦称之为业务中台）。 其二则是工程问题了，即沟通成本的问题。举个例子，HTML 语言提供了 button 标签，但这个标签除了渲染一个按钮外，还会同时渲染一个文本输入框给我们。这显然不是我们需要的，好在这个标签同时提供了一个配置给我们，可能显式地指定避免渲染文本框（额外的工作是你需要指定该配置 &lt;button disable-input=&#39;true&#39;&gt;click&lt;/button&gt;）。这样子虽然最终实现了想要的效果，但却让代码变得不优雅，不仅你花费了不必要的时间，还导致后来的维护者看到会迷惑，沟通成本由此产生。 尝试把这个现象无限放大，如果你在代码中看到的每一个概念定义都是不可信的，都是与现实世界有偏差的，那么当你要解决一个问题或开发一个新功能的时候，你工作中花在的这些无谓的沟通中的时间就会成倍地提升（尤其是当没有人或文档能解答你的问题的时候），最终远远大于你的编码时间。 问题分析到这，答案也就出来了。如何治理一个复杂的软件系统？私以为关键在于做好模型设计，以及维护好概念一致性。前者可以通过组合逻辑空间的不同维度，使得复杂度的表示难度指数下降，最终达到人脑可以处理的水平。后者则保证模型中的概念跟现实世界是一致的，易于理解，可减少不必要的沟通成本。两者任一没有处理好，均会导致复杂度指数上升，进而导致工作量增大，因此程序猿加班也就成了常态了。经常加班的猿们，都应该先反问下自己，这两点是不是都做得足够好了？如果是，再来问是不是人手不够之类的问题。 当然现实往往更复杂，很多同学都会以业务方不给时间，上级不支持为由，写出一堆烂代码来。确实这也是一些阻力，但并不能成为人云亦云的借口。有这种想法的同学，要么是选择躺平了，不想去改变现状；要么是缺乏对事物发展规律的认识的，推荐学习一下教员的《矛盾论》。]]></content>
      <categories>
        <category>个人思考</category>
        <category>软件工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一个很好用的Spring Boot Starter Swagger]]></title>
    <url>%2Fmy_swagger_spring_boot_starter.html</url>
    <content type="text"><![CDATA[Spring Boot Starter Swagger简介Swagger与Spring Boot现在在Java Web开发领域是再常用不过的两个框架了。集成这两者的Starter现在在Github上也存在很多（基本都是非官方的，官方好像没有提供Starter），但是大多数或多或少都存在以下问题： 整合程度低，许多Springfox-Swagger2提供的功能无法通过Spring Boot的方式进行配置或者配置方式复杂 项目疏于维护，许多Issue没人去解决 扩展性不足，当用户出现一些需求时只能祈求项目更新或者自行修改源码 无法同时兼容Spring Boot1.x和Spring Boot2.x 我曾在Github上找了许久都没找到，索性自行开发了一个。 这个Starter具有以下特点： 完美适配springfox-swagger2，几乎支持通过yaml文件进行所有配置 提供拦截器，允许用户自行扩展自定义配置 同时支持spring boot1和spring boot2 扩展了一些小功能，如展示当前hostname等 项目地址 如何使用引入依赖 pom.xml &lt;!-- spring boot1用户 --> &lt;dependency> &lt;groupId>com.github.taccisum&lt;/groupId> &lt;artifactId>swagger-spring-boot1-starter&lt;/artifactId> &lt;version>{lastest.version}&lt;/version> &lt;/dependency> &lt;!-- spring boot2用户 --> &lt;dependency> &lt;groupId>com.github.taccisum&lt;/groupId> &lt;artifactId>swagger-spring-boot2-starter&lt;/artifactId> &lt;version>{lastest.version}&lt;/version> &lt;/dependency> application.yml swagger: base-package: com.github.taccisum.controller 启动项目，打开 http://localhost:8080/swagger-ui.html 即可查看API文档。 更多功能可以到 https://github.com/taccisum/spring-boot-starter-swagger 了解。]]></content>
      <categories>
        <category>framework</category>
        <category>starter</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[flowable启动流程解析（spring boot）]]></title>
    <url>%2Fsourece%2Fflowable%2Fstartup%2Fspring_boot.html</url>
    <content type="text"><![CDATA[简介flowable engine一共有5种：App, CMMN, DMN, Form, Process(即BPMN)。 flowable-spring-boot-starter提供了零配置集成flowable的功能，主要包括各类型engine的自动配置、流程/表单定义自动部署，其次还有rest-api，spring boot aucuator集成等。 这篇文章主要是解析flowable在spring boot环境下的启动流程，不涉及flowable内部原理。 flowable相关的AutoConfiguration# flowable-spring-boot-autoconfigure: spring.factories org.springframework.boot.env.EnvironmentPostProcessor=\ org.flowable.spring.boot.environment.FlowableDefaultPropertiesEnvironmentPostProcessor,\ org.flowable.spring.boot.environment.FlowableLiquibaseEnvironmentPostProcessor # Flowable auto-configurations org.springframework.boot.autoconfigure.EnableAutoConfiguration=\ org.flowable.spring.boot.actuate.info.FlowableInfoAutoConfiguration,\ org.flowable.spring.boot.EndpointAutoConfiguration,\ org.flowable.spring.boot.RestApiAutoConfiguration,\ org.flowable.spring.boot.app.AppEngineServicesAutoConfiguration,\ org.flowable.spring.boot.app.AppEngineAutoConfiguration,\ org.flowable.spring.boot.ProcessEngineServicesAutoConfiguration,\ org.flowable.spring.boot.ProcessEngineAutoConfiguration,\ org.flowable.spring.boot.FlowableJpaAutoConfiguration,\ org.flowable.spring.boot.form.FormEngineAutoConfiguration,\ org.flowable.spring.boot.form.FormEngineServicesAutoConfiguration,\ org.flowable.spring.boot.content.ContentEngineAutoConfiguration,\ org.flowable.spring.boot.content.ContentEngineServicesAutoConfiguration,\ org.flowable.spring.boot.dmn.DmnEngineAutoConfiguration,\ org.flowable.spring.boot.dmn.DmnEngineServicesAutoConfiguration,\ org.flowable.spring.boot.idm.IdmEngineAutoConfiguration,\ org.flowable.spring.boot.idm.IdmEngineServicesAutoConfiguration,\ org.flowable.spring.boot.cmmn.CmmnEngineAutoConfiguration,\ org.flowable.spring.boot.cmmn.CmmnEngineServicesAutoConfiguration,\ org.flowable.spring.boot.ldap.FlowableLdapAutoConfiguration,\ org.flowable.spring.boot.FlowableSecurityAutoConfiguration 看起来比较多，不过大多数AutoConfiguration逻辑还是比较简单的。 engine自动配置各engine的配置方式大同小异，以ProcessEngine为例，其涉及到的AutoConfiguration主要是 ProcessEngineAutoConfiguration ProcessEngineServicesAutoConfiguration ProcessEngineAutoConfigurationProcessEngineAutoConfiguration类图 从红框部分可以看到，在ProcessEngineAutoConfiguration中配置了一个类型为SpringProcessEngineConfiguration的bean，其类图如下 可知，SpringProcessEngineConfiguration是ProcessEngineConfiguration的子类，而ProcessEngineConfiguration正是用于创建ProcessEngine的类。 不过这里只是注册了一个bean，并没有调用其buildProcessEngine()方法来创建ProcessEngine。ProcessEngine实例是在ProcessEngineFactoryBean中创建的。 ProcessEngineServicesAutoConfiguration这个AutoConfiguration主要负责配置ProcessEngineFactoryBean及各个Service（RuntimeService, RepositoryService, TaskService等）。 各Service的配置比较简单，主要来看看ProcessEngineFactoryBean ProcessEngineFactoryBean需要注意的是ProcessEngineFactoryBean是在内部类ProcessEngineServicesAutoConfiguration#StandaloneEngineConfiguration中注册的。 这是一个FactoryBean，我们知道Spring通过FactoryBean的getObject()方法来创建bean，来看看其代码 // ProcessEngineFactoryBean.java public class ProcessEngineFactoryBean implements FactoryBean&lt;ProcessEngine>, DisposableBean, ApplicationContextAware { protected ProcessEngineConfigurationImpl processEngineConfiguration; @Override public ProcessEngine getObject() throws Exception { // 省略无关代码... this.processEngine = processEngineConfiguration.buildProcessEngine(); return this.processEngine; } } 可以看到，ProcessEngineFactoryBean通过调用processEngineConfiguration.buildProcessEngine()创建了ProcessEngine的实例。 对于processEngineConfiguration这个对象的构建，可以参考ProcessEngineAutoConfiguration 自动部署flowable spring boot starter能够自动将classpath的相关目录（如processes, forms）下的资源自动部署。 不同的engine逻辑大同小异，以ProcessEngine为例，其核心在于SpringProcessEngineConfiguration这个类。 SpringProcessEngineConfiguration实现了spring的SmartLifecycle接口，相关代码如下 // SpringProcessEngineConfiguration.java @Override public void start() { synchronized (lifeCycleMonitor) { if (!isRunning()) { // 遍历engines实例进行部署 enginesBuild.forEach(name -> autoDeployResources(ProcessEngines.getProcessEngine(name))); running = true; } } } @Override public void stop() { synchronized (lifeCycleMonitor) { running = false; } } @Override public boolean isRunning() { return running; } 其中start方法正是对process engines所需要的资源进行自动部署，会在spring应用完成初始化后进行回调。 来看看autoDeployResources方法 // SpringProcessEngineConfiguration.java protected Resource[] deploymentResources = new Resource[0]; protected void autoDeployResources(ProcessEngine processEngine) { if (deploymentResources != null &amp;&amp; deploymentResources.length > 0) { final AutoDeploymentStrategy strategy = getAutoDeploymentStrategy(deploymentMode); // 选择部署策略 strategy.deployResources(deploymentName, deploymentResources, processEngine.getRepositoryService()); } } 字段deploymentResources的值是关键，通过调试，发现该字段是在ProcessEngineAutoConfiguration中进行赋值的 // ProcessEngineAutoConfiguration @Bean @ConditionalOnMissingBean public SpringProcessEngineConfiguration springProcessEngineConfiguration(DataSource dataSource, PlatformTransactionManager platformTransactionManager, @Process ObjectProvider&lt;IdGenerator> processIdGenerator, ObjectProvider&lt;IdGenerator> globalIdGenerator, @ProcessAsync ObjectProvider&lt;AsyncExecutor> asyncExecutorProvider, @ProcessAsyncHistory ObjectProvider&lt;AsyncExecutor> asyncHistoryExecutorProvider) throws IOException { SpringProcessEngineConfiguration conf = new SpringProcessEngineConfiguration(); // 根据配置的规则找到相关的资源 List&lt;Resource> resources = this.discoverDeploymentResources( flowableProperties.getProcessDefinitionLocationPrefix(), flowableProperties.getProcessDefinitionLocationSuffixes(), flowableProperties.isCheckProcessDefinitions() ); if (resources != null &amp;&amp; !resources.isEmpty()) { conf.setDeploymentResources(resources.toArray(new Resource[0])); conf.setDeploymentName(flowableProperties.getDeploymentName()); } // ...省略无关代码 return conf; } TODO LIST List&lt;EngineConfigurationConfigurer>]]></content>
      <categories>
        <category>source</category>
        <category>flowable</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— 动态加载Filter（未完成）]]></title>
    <url>%2Fsource%2Fzuul%2Ffilter%2Fdynamic_load.html</url>
    <content type="text"><![CDATA[介绍zuul支持用两种语言编写的过滤器，分别是Groovy和Java。不过只有Groovy编写的过滤器才支持动态加载。 所谓动态加载，即是可以在应用的运行时对filter进行CRUD的操作，以达到动态调整zuul行为的目的。 以下我们看看zuul是如何实现这一点的。 FilterLoader// FilterLoader.java /** * 获取所有指定类型的filter并排序，通过ZuulFilter.filterOrder()方法 * Returns a list of filters by the filterType specified */ public List&lt;ZuulFilter> getFiltersByType(String filterType) { // 尝试从缓存中获取，如果存在缓存，则直接返回 List&lt;ZuulFilter> list = hashFiltersByType.get(filterType); if (list != null) return list; list = new ArrayList&lt;ZuulFilter>(); // 从registry获取所有filter，从中找出需要的filter，最终进行排序 Collection&lt;ZuulFilter> filters = filterRegistry.getAllFilters(); for (Iterator&lt;ZuulFilter> iterator = filters.iterator(); iterator.hasNext(); ) { ZuulFilter filter = iterator.next(); if (filter.filterType().equals(filterType)) { list.add(filter); } } Collections.sort(list); // sort by priority // 将结果缓存起来 hashFiltersByType.putIfAbsent(filterType, list); return list; } 从上面代码可以看到，getFiltersByType()依赖于对象filterRegistry，zuul正是通过对该registry的CRUD实现动态加载filter的功能。 filterRegistry是FilterRegistry的一个唯一实例（单例模式）。FilterRegistry的代码很简单，就是简单地维护了一个用于存放filter的ConcurrentHashMap而以。关键需要找出zuul是如何维护这份registry的。 FilterScriptManagerServlet需要在运行时维护registry，必然需要有一个入口。这个入口就是FilterScriptManagerServlet，这是一个HttpServlet，提供了以下HTTP资源： 路径 请求方式 Servlet对应的处理方法 描述 LIST GET handleListAction 获取filter脚本 DOWNLOAD GET handleDownloadAction 下载脚本 UPLOAD PUT/POST handleUploadAction 上传脚本 ACTIVATE PUT/POST handleActivateAction 启用脚本 CANARY PUT/POST handleCanaryAction TODO::还不知道干啥用的 DEACTIVATE PUT/POST handledeActivateAction 禁用脚本]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— Post类型过滤器]]></title>
    <url>%2Fsource%2Fzuul%2Ffilters%2Fpost.html</url>
    <content type="text"><![CDATA[Post类型过滤器Post类型过滤器执行顺序为：Postfilter(10) -&gt; RequestEventInfoCollectorFilter(99) -&gt; sendResponse(1000) -&gt; Stats(2000) Postfilter后置处理过滤器，主要负责对响应内容进行修饰（如添加响应头）。 // PostDecoration.groovy boolean shouldFilter() { // 判断请求是否是从zuul路由到自身的 if (true.equals(NFRequestContext.getCurrentContext().zuulToZuul)) return false; //request was routed to a zuul server, so don't send response headers return true } Object run() { // 为响应添加一些header addStandardResponseHeaders(RequestContext.getCurrentContext().getRequest(), RequestContext.getCurrentContext().getResponse()) return null; } void addStandardResponseHeaders(HttpServletRequest req, HttpServletResponse res) { println(originatingURL) String origin = req.getHeader(ORIGIN) // 没用到 RequestContext context = RequestContext.getCurrentContext() List&lt;Pair&lt;String, String>> headers = context.getZuulResponseHeaders() headers.add(new Pair(X_ZUUL, "zuul")) // X-Zuul headers.add(new Pair(X_ZUUL_INSTANCE, System.getenv("EC2_INSTANCE_ID") ?: "unknown")) // X-Zuul-instance headers.add(new Pair(CONNECTION, KEEP_ALIVE)) // Connection headers.add(new Pair(X_ZUUL_FILTER_EXECUTION_STATUS, context.getFilterExecutionSummary().toString())) // X-Zuul-Filter-Executions headers.add(new Pair(X_ORIGINATING_URL, originatingURL)) // X-Originating-URL if (context.get("ErrorHandled") == null &amp;&amp; context.responseStatusCode >= 400) { headers.add(new Pair(X_NETFLIX_ERROR_CAUSE, "Error from Origin")) // X-Netflix-Error-Cause ErrorStatsManager.manager.putStats(RequestContext.getCurrentContext().route, "Error_from_Origin_Server") } } RequestEventInfoCollectorFilter这个过滤器负责收集要发送到ESI, EventBus, Turbine等的数据，例如此次请求的数据和当前应用实例的数据。 TODO:: 虽然zuul收集了这些数据，但是并没有找到在哪里使用… mark一下 // RequestEventInfoCollector.groovy boolean shouldFilter() { return true } Object run() { NFRequestContext ctx = NFRequestContext.getCurrentContext(); final Map&lt;String, Object> event = ctx.getEventProperties(); try { // 往eventProperties中写入与此次请求有关的数据 captureRequestData(event, ctx.request); // 往eventProperties中写入与当前实例有关的数据 captureInstanceData(event); } catch (Exception e) { event.put("exception", e.toString()); LOG.error(e.getMessage(), e); } } capture data这两个方法比较啰嗦，不过逻辑并不复杂，就是收集各种数据并写入map中 // RequestEventInfoCollector.groovy void captureRequestData(Map&lt;String, Object> event, HttpServletRequest req) { try { // 写入请求基本信息 // basic request properties event.put("path", req.getPathInfo()); event.put("host", req.getHeader("host")); event.put("query", req.getQueryString()); event.put("method", req.getMethod()); event.put("currentTime", System.currentTimeMillis()); // 写入请求头 // request headers for (final Enumeration names = req.getHeaderNames(); names.hasMoreElements();) { final String name = names.nextElement(); final StringBuilder valBuilder = new StringBuilder(); boolean firstValue = true; for (final Enumeration vals = req.getHeaders(name); vals.hasMoreElements();) { // only prepends separator for non-first header values if (firstValue) firstValue = false; else { valBuilder.append(VALUE_SEPARATOR); } valBuilder.append(vals.nextElement()); } event.put("request.header." + name, valBuilder.toString()); } // 写入请求参数 // request params final Map params = req.getParameterMap(); for (final Object key : params.keySet()) { final String keyString = key.toString(); final Object val = params.get(key); String valString; if (val instanceof String[]) { final String[] valArray = (String[]) val; if (valArray.length == 1) valString = valArray[0]; else valString = Arrays.asList((String[]) val).toString(); } else { valString = val.toString(); } event.put("param." + key, valString); // some special params get promoted to top-level fields if (keyString.equals("esn")) { event.put("esn", valString); } } // 写入响应头 // response headers NFRequestContext.getCurrentContext().getZuulResponseHeaders()?.each { Pair&lt;String, String> it -> event.put("response.header." + it.first().toLowerCase(), it.second()) } } finally { } } private static final void captureInstanceData(Map&lt;String, Object> event) { try { final String stack = ConfigurationManager.getDeploymentContext().getDeploymentStack(); if (stack != null) event.put("stack", stack); // TODO: add CLUSTER, ASG, etc. // 获取此实例（zuul）的信息 final InstanceInfo instanceInfo = ApplicationInfoManager.getInstance().getInfo(); // 写入实例信息，id和metadata等 if (instanceInfo != null) { event.put("instance.id", instanceInfo.getId()); for (final Map.Entry&lt;String, String> e : instanceInfo.getMetadata().entrySet()) { event.put("instance." + e.getKey(), e.getValue()); } } // AWS相关，跳过 // caches value after first call. multiple threads could get here simultaneously, but I think that is fine final AmazonInfo amazonInfo = AmazonInfoHolder.getInfo(); for (final Map.Entry&lt;String, String> e : amazonInfo.getMetadata().entrySet()) { event.put("amazon." + e.getKey(), e.getValue()); } } finally { } } sendResponsesendResponse是比较重要的一个过滤器，负责将响应写回请求来源。 // sendResponse.groovy boolean shouldFilter() { return !RequestContext.currentContext.getZuulResponseHeaders().isEmpty() || RequestContext.currentContext.getResponseDataStream() != null || RequestContext.currentContext.responseBody != null } Object run() { // 添加一些响应头并收集响应debug信息到上下文 addResponseHeaders() // 写响应流 writeResponse() } void writeResponse() { RequestContext context = RequestContext.currentContext // there is no body to send if (context.getResponseBody() == null &amp;&amp; context.getResponseDataStream() == null) return; HttpServletResponse servletResponse = context.getResponse() servletResponse.setCharacterEncoding("UTF-8") OutputStream outStream = servletResponse.getOutputStream(); InputStream is = null try { // 如果上下文中设置了responseBody，则覆盖掉原来的输出 if (RequestContext.currentContext.responseBody != null) { String body = RequestContext.currentContext.responseBody writeResponse(new ByteArrayInputStream(body.getBytes(Charset.forName("UTF-8"))), outStream) return; } // 判断请求是否能接收gzip压缩 boolean isGzipRequested = false final String requestEncoding = context.getRequest().getHeader(ZuulHeaders.ACCEPT_ENCODING) if (requestEncoding != null &amp;&amp; requestEncoding.equals("gzip")) isGzipRequested = true; is = context.getResponseDataStream(); InputStream inputStream = is if (is != null) { if (context.sendZuulResponse()) { // if origin response is gzipped, and client has not requested gzip, decompress stream // before sending to client // else, stream gzip directly to client // 如果原始响应是gzip压缩的，但请求来源并不接受gzip，就解压后再返回，否则直接返回gzip响应 if (context.getResponseGZipped() &amp;&amp; !isGzipRequested) try { inputStream = new GZIPInputStream(is); } catch (java.util.zip.ZipException e) { println("gzip expected but not received assuming unencoded response" + RequestContext.currentContext.getRequest().getRequestURL().toString()) inputStream = is } else if (context.getResponseGZipped() &amp;&amp; isGzipRequested) servletResponse.setHeader(ZuulHeaders.CONTENT_ENCODING, "gzip") writeResponse(inputStream, outStream) } } } finally { try { is?.close(); outStream.flush() outStream.close() } catch (IOException e) { } } } // 其它方法比较简单，省略了... Stats负责收集统计数据，以及将filter执行过程中收集到的debug信息输出到控制台。 @Override boolean shouldFilter() { return true } @Override Object run() { int status = RequestContext.getCurrentContext().getResponseStatusCode(); StatsManager sm = StatsManager.manager // 收集统计数据 sm.collectRequestStats(RequestContext.getCurrentContext().getRequest()); sm.collectRouteStats(RequestContext.getCurrentContext().route, status); // 打印debug信息 dumpRoutingDebug() dumpRequestDebug() }]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— Route类型过滤器]]></title>
    <url>%2Fsource%2Fzuul%2Ffilters%2Froute.html</url>
    <content type="text"><![CDATA[Route类型过滤器Route类型的过滤器在zuul中负责对请求进行转发，zuul作为网关的最核心的功能就体现在route类型过滤器中。Netflix提供了两个route类型的过滤器：ZuulHostRequest和ZuulNFRequest，但任意请求只会满足其中一个过滤器的执行条件。 Route类型过滤器执行顺序为：ZuulNFRequest(10) -&gt; ZuulHostRequest(100) ZuulHostRequestZuulHostRequest简单地根据host来转发请求，host的值根据上下文变量routeHost取得。 // ZuulHostRequest.groovy boolean shouldFilter() { // 如果routeHost不为空，则说明此次请求是转发到指定host，由此过滤器进行处理，否则由ZuulNFRequest进行处理 return RequestContext.currentContext.getRouteHost() != null &amp;&amp; RequestContext.currentContext.sendZuulResponse() } Object run() { HttpServletRequest request = RequestContext.currentContext.getRequest(); // 构建请求headers，会包括原请求请求头和zuul添加的请求头 Header[] headers = buildZuulRequestHeaders(request) // 获取HTTP动词，即GET/POST之类 String verb = getVerb(request); InputStream requestEntity = getRequestBody(request) HttpClient httpclient = CLIENT.get() String uri = request.getRequestURI() if (RequestContext.currentContext.requestURI != null) { // 如果在之前的过滤器中为上下文添加了requestURI，则覆盖掉原uri uri = RequestContext.currentContext.requestURI } try { // 转发请求并将响应保存到上下文 HttpResponse response = forward(httpclient, verb, uri, request, headers, requestEntity) setResponse(response) } catch (Exception e) { throw e; } return null } ZuulHostRequest的run方法会根据原请求和上下文构建一个新的HTTP请求，然后进行转发，接下来看看forward方法 private static final AtomicReference&lt;HttpClient> CLIENT = new AtomicReference&lt;HttpClient>(newClient()); HttpResponse forward(HttpClient httpclient, String verb, String uri, HttpServletRequest request, Header[] headers, InputStream requestEntity) { // 如果开启了debugRequest的话，这里会返回一个wrap过的requestEntity(debugRequestEntity)用于debug requestEntity = debug(httpclient, verb, uri, request, headers, requestEntity) org.apache.http.HttpHost httpHost httpHost = getHttpHost() org.apache.http.HttpRequest httpRequest; switch (verb) { case 'POST': httpRequest = new HttpPost(uri + getQueryString()) InputStreamEntity entity = new InputStreamEntity(requestEntity, request.getContentLength()) httpRequest.setEntity(entity) break case 'PUT': httpRequest = new HttpPut(uri + getQueryString()) InputStreamEntity entity = new InputStreamEntity(requestEntity, request.getContentLength()) httpRequest.setEntity(entity) break; default: httpRequest = new BasicHttpRequest(verb, uri + getQueryString()) } try { httpRequest.setHeaders(headers) HttpResponse zuulResponse = executeHttpRequest(httpclient, httpHost, httpRequest) return zuulResponse } finally { // When HttpClient instance is no longer needed, // shut down the connection manager to ensure // immediate deallocation of all system resources // 这行被注释掉了，可能因为之前用的client不是静态变量，现在换成静态变量后就不需要在这里释放连接了 // httpclient.getConnectionManager().shutdown(); } } HttpResponse executeHttpRequest(HttpClient httpclient, HttpHost httpHost, HttpRequest httpRequest) { // 封装成hystrix command执行，有关hystrix的内容这里不做讨论 HostCommand command = new HostCommand(httpclient, httpHost, httpRequest) command.execute(); } ZuulNFRequestZuulNFRequest可以将请求路由到注册到eureka server的服务中。它使用Ribbon客户端，可以将请求就当前可用实例进行负载均衡。 // ZuulNFRequest.groovy boolean shouldFilter() { return NFRequestContext.currentContext.getRouteHost() == null &amp;&amp; RequestContext.currentContext.sendZuulResponse() } Object run() { NFRequestContext context = NFRequestContext.currentContext HttpServletRequest request = context.getRequest(); // 构建请求headers，会包括原请求请求头和zuul添加的请求头 MultivaluedMap&lt;String, String> headers = buildZuulRequestHeaders(request) // 构建请求参数 MultivaluedMap&lt;String, String> params = buildZuulRequestQueryParams(request) Verb verb = getVerb(request); Object requestEntity = getRequestBody(request) // 通过routeVIP获取到相应的ribbon客户端 IClient restClient = ClientFactory.getNamedClient(context.getRouteVIP()); String uri = request.getRequestURI() if (context.requestURI != null) { uri = context.requestURI } //remove double slashes uri = uri.replace("//", "/") HttpResponse response = forward(restClient, verb, uri, headers, params, requestEntity) setResponse(response) return response } 逻辑基本上与ZuulHostRequest一致，唯一不同的是使用的HTTP客户端不一样。ZuulNFRequest使用的客户端是从ClientFactory中获取的命名客户端(named client)，每个命名客户端会有一个相应的命名配置(named config)，这意味者每个客户端都可以有不同的配置。来看看相关代码 tip: 这个ClientFactory其实是属于ribbon的类，如果对ribbon有了解可以选择跳过下面的内容不看。 // ClientFactory.java public static synchronized IClient getNamedClient(String name, Class&lt;? extends IClientConfig> configClass) { if (simpleClientMap.get(name) != null) { return simpleClientMap.get(name); } try { // client不存在，尝试创建一个 return createNamedClient(name, configClass); } catch (ClientException e) { throw new RuntimeException("Unable to create client", e); } } public static synchronized IClient createNamedClient(String name, Class&lt;? extends IClientConfig> configClass) throws ClientException { // 获取命名配置 IClientConfig config = getNamedConfig(name, configClass); return registerClientFromProperties(name, config); } 需要注意到的是在createNamedClient()中，传入的client config是一个类型，通过getNamedConfig(name, configClass)获取到当前客户端对应的命名配置。 命名配置所谓命名配置即是为每份配置起一个名字，然后在运行时就可以根据不同的名字来获取不同的配置，例如我们可以这样配置 # zuul.properties origin.zuul.client.DeploymentContextBasedVipAddresses=ORIGIN origin.zuul.client.Port=8080 foo.zuul.client.DeploymentContextBasedVipAddresses=FOO foo.zuul.client.Port=8081 bar.zuul.client.DeploymentContextBasedVipAddresses=BAR bar.zuul.client.Port=8082 然后构造出来的origin, foo, bar这三个客户端对应的配置分别是vip: ORIGIN, FOO, BAR; port: 8080, 8081, 8082 获取命名配置的代码如下 // ClientFactory.java public static IClientConfig getNamedConfig(String name, Class&lt;? extends IClientConfig> clientConfigClass) { IClientConfig config = namedConfig.get(name); if (config != null) { return config; } else { try { config = (IClientConfig) clientConfigClass.newInstance(); // 以名称前缀加载相应的配置 config.loadProperties(name); } catch (Throwable e) { logger.error("Unable to create client config instance", e); return null; } config.loadProperties(name); IClientConfig old = namedConfig.putIfAbsent(name, config); if (old != null) { config = old; } return config; } }]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— Pre和其它类型的过滤器]]></title>
    <url>%2Fsource%2Fzuul%2Ffilters%2Fpre.html</url>
    <content type="text"><![CDATA[Pre类型Pre类型的过滤器执行顺序为：DebugFilter(1) -&gt; Routing(1) -&gt; PreDecoration(20) -&gt; WeightedLoadBalancer(30) -&gt; DebugRequest(10000) DebugFilter判断是否为此次请求开启debug的一个过滤器。 // Debug.groovy boolean shouldFilter() { // 配置中是否开启debug if ("true".equals(RequestContext.currentContext.getRequest().getParameter(debugParameter.get()))) return true; return routingDebug.get(); } Object run() { // 设置当前请求的上下文的debug标识为true，作为之后执行filter时是否记录debug信息的依据 RequestContext.getCurrentContext().setDebugRequest(true) RequestContext.getCurrentContext().setDebugRouting(true) return null; } RoutingRouting过滤器判断是将请求路由到静态资源还是其它服务。 // Routing.groovy boolean shouldFilter() { return true } Object staticRouting() { // 路由到静态资源 FilterProcessor.instance.runFilters("healthcheck") FilterProcessor.instance.runFilters("static") } Object run() { staticRouting() //runs the static Zuul // TODO:: 这里routeVIP的值是固定的（origin），后面也没有找到修改该值的地方，导致zuul只能路由到某个单一的服务，暂时不知道原因，先mark一下 // 目标Eureka VIP ((NFRequestContext) RequestContext.currentContext).routeVIP = defaultClient.get() String host = defaultHost.get() if (((NFRequestContext) RequestContext.currentContext).routeVIP == null) ((NFRequestContext) RequestContext.currentContext).routeVIP = ZuulApplicationInfo.applicationName if (host != null) { final URL targetUrl = new URL(host) RequestContext.currentContext.setRouteHost(targetUrl); ((NFRequestContext) RequestContext.currentContext).routeVIP = null } // host与routeVIP不能同时为null if (host == null &amp;&amp; RequestContext.currentContext.routeVIP == null) { throw new ZuulException("default VIP or host not defined. Define: zuul.niws.defaultClient or zuul.default.host", 501, "zuul.niws.defaultClient or zuul.default.host not defined") } String uri = RequestContext.currentContext.request.getRequestURI() // 如果在之前的filter当中给上下文的requestURI赋值了，则覆盖原uri的值 if (RequestContext.currentContext.requestURI != null) { uri = RequestContext.currentContext.requestURI } if (uri == null) uri = "/" if (uri.startsWith("/")) { uri = uri - "/" } // 截取路径的第一段为route // TODO:: 这个route有什么用，也暂时没发现 ((NFRequestContext) RequestContext.currentContext).route = uri.substring(0, uri.indexOf("/") + 1) } PreDecoration预处理过滤器，负责对请求做一些预处理操作，如添加请求头。 // PreDecoration.groovy @Override boolean shouldFilter() { return true } @Override Object run() { if (RequestContext.currentContext.getRequest().getParameter("url") != null) { try { // routeHost通过请求的参数url指定 // 如果routeHost有值，则在route阶段会由ZuulHostRequest进行处理 RequestContext.getCurrentContext().routeHost = new URL(RequestContext.currentContext.getRequest().getParameter("url")) // 开启GZip RequestContext.currentContext.setResponseGZipped(true) } catch (MalformedURLException e) { // url格式错误，返回400 throw new ZuulException(e, "Malformed URL", 400, "MALFORMED_URL") } } setOriginRequestHeaders() return null } void setOriginRequestHeaders() { RequestContext context = RequestContext.currentContext context.addZuulRequestHeader("X-Netflix.request.toplevel.uuid", UUID.randomUUID().toString()) // 添加被代理者的ip地址到XFF context.addZuulRequestHeader(X_FORWARDED_FOR, context.getRequest().remoteAddr) // 设置Host头 context.addZuulRequestHeader(X_NETFLIX_CLIENT_HOST, context.getRequest().getHeader(HOST)) if (context.getRequest().getHeader(X_FORWARDED_PROTO) != null) { context.addZuulRequestHeader(X_NETFLIX_CLIENT_PROTO, context.getRequest().getHeader(X_FORWARDED_PROTO)) } } WeightedLoadBalanceTODO:: 加权负载均衡器，似乎与金丝雀发布（灰度发布）有关，暂不深入了解。 DebugRequest负责添加Request的debug信息的过滤器 // DebugRequest.groovy @Override boolean shouldFilter() { return Debug.debugRequest() } @Override Object run() { // 获取原始请求 HttpServletRequest req = RequestContext.currentContext.request as HttpServletRequest // 收集客户端ip信息 Debug.addRequestDebug("REQUEST:: " + req.getScheme() + " " + req.getRemoteAddr() + ":" + req.getRemotePort()) // 收集HTTP请求行信息 Debug.addRequestDebug("REQUEST:: > " + req.getMethod() + " " + req.getRequestURI() + " " + req.getProtocol()) // 收集原请求的请求头信息 Iterator headerIt = req.getHeaderNames().iterator() while (headerIt.hasNext()) { String name = (String) headerIt.next() String value = req.getHeader(name) Debug.addRequestDebug("REQUEST:: > " + name + ":" + value) } // 收集原请求的请求体信息 final RequestContext ctx = RequestContext.getCurrentContext() if (!ctx.isChunkedRequestBody()) { InputStream inp = ctx.request.getInputStream() String body = null if (inp != null) { body = inp.getText() Debug.addRequestDebug("REQUEST:: > " + body) } } return null; } 打印出来的调试信息类似下面这样： REQUEST_DEBUG::REQUEST:: http 0:0:0:0:0:0:0:1:54075 REQUEST_DEBUG::REQUEST:: > GET /auth-center/foo/bar HTTP/1.1 REQUEST_DEBUG::REQUEST:: > Host:localhost:8080 REQUEST_DEBUG::REQUEST:: > Connection:keep-alive REQUEST_DEBUG::REQUEST:: > Cache-Control:max-age=0 REQUEST_DEBUG::REQUEST:: > Upgrade-Insecure-Requests:1 REQUEST_DEBUG::REQUEST:: > User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36 REQUEST_DEBUG::REQUEST:: > Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8 REQUEST_DEBUG::REQUEST:: > Accept-Encoding:gzip, deflate, br REQUEST_DEBUG::REQUEST:: > Accept-Language:zh-CN,zh;q=0.9,zh-TW;q=0.8 REQUEST_DEBUG::REQUEST:: > Cookie:Idea-9d03e8f=36659dd1-27e5-4e9b-8824-19bf2de30b9e; _ga=GA1.1.79701639.1521035841; wcsid=u5MHraIxIjs3Na0G3m39N0Hab53odbCD; hblid=5HLUbXnG2OuboFyp3m39N0HbjAa3Cd5a; _oklv=1542679881769%2Cu5MHraIxIjs3Na0G3m39N0Hab53odbCD; _okdetect=%7B%22token%22%3A%2215426798825920%22%2C%22proto%22%3A%22http%3A%22%2C%22host%22%3A%22localhost%3A4040%22%7D; olfsk=olfsk020058268955480463; _okbk=cd4%3Dtrue%2Cvi5%3D0%2Cvi4%3D1542679883339%2Cvi3%3Dactive%2Cvi2%3Dfalse%2Cvi1%3Dfalse%2Ccd8%3Dchat%2Ccd6%3D0%2Ccd5%3Daway%2Ccd3%3Dfalse%2Ccd2%3D0%2Ccd1%3D0%2C; _ok=1700-237-10-3483; _gauges_unique_month=1; _gauges_unique_year=1; _gauges_unique=1; freeform=3213 REQUEST_DEBUG::REQUEST:: > 其它类型Options好像是一个没用的filter…感觉是还未完成？ // Options.groovy boolean shouldFilter() { String method = RequestContext.currentContext.getRequest() getMethod(); // 处理OPTIONS方法的HTTP请求 if (method.equalsIgnoreCase("options")) return true; } @Override String uri() { return "any path here" } @Override String responseBody() { // 啥也不返回 return "" // empty response } HealthcheckHealthcheck过滤器提供了一个静态资源/healthcheck，方便检测zuul应用是否正常运行。但是功能好像过于弱鸡了… - - // Healthcheck.groovy @Override String filterType() { return "healthcheck" } @Override String uri() { return "/healthcheck" } @Override String responseBody() { // 只返回了一个ok...简单粗暴... RequestContext.getCurrentContext().getResponse().setContentType('application/xml') return "&lt;health>ok&lt;/health>" } ErrorResponse// ErrorResponse.groovy boolean shouldFilter() { // 根据标识判断错误是否已被处理 return RequestContext.getCurrentContext().get("ErrorHandled") == null } Object run() { RequestContext context = RequestContext.currentContext Throwable ex = context.getThrowable() try { LOG.error(ex.getMessage(), ex); throw ex } catch (ZuulException e) { String cause = e.errorCause if (cause == null) cause = "UNKNOWN" // 添加错误原因到请求头X-Netflix-Error-Cause RequestContext.getCurrentContext().getResponse().addHeader("X-Netflix-Error-Cause", "Zuul Error: " + cause) // 对该次错误请求进行统计 if (e.nStatusCode == 404) { ErrorStatsManager.manager.putStats("ROUTE_NOT_FOUND", "") } else { ErrorStatsManager.manager.putStats(RequestContext.getCurrentContext().route, "Zuul_Error_" + cause) } // 判断是否改写响应状态，则请求传入的参数决定 if (overrideStatusCode) { RequestContext.getCurrentContext().setResponseStatusCode(200); } else { RequestContext.getCurrentContext().setResponseStatusCode(e.nStatusCode); } // 设置标识，表示不再返回zuul转发请求得到的响应结果（如果有） context.setSendZuulResponse(false) // 设置zuul的异常响应body context.setResponseBody("${getErrorMessage(e, e.nStatusCode)}") } catch (Throwable throwable) { // 处理未知异常，与处理ZuulException的逻辑大体相同 RequestContext.getCurrentContext().getResponse().addHeader("X-Zuul-Error-Cause", "Zuul Error UNKNOWN Cause") ErrorStatsManager.manager.putStats(RequestContext.getCurrentContext().route, "Zuul_Error_UNKNOWN_Cause") if (overrideStatusCode) { RequestContext.getCurrentContext().setResponseStatusCode(200); } else { RequestContext.getCurrentContext().setResponseStatusCode(500); } context.setSendZuulResponse(false) context.setResponseBody("${getErrorMessage(throwable, 500)}") } finally { // 设置标识，表示错误已经被处理（防止存在多个error过滤器时重复处理） context.set("ErrorHandled") //ErrorResponse was handled return null; } }]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— 过滤器]]></title>
    <url>%2Fsource%2Fzuul%2Ffilters.html</url>
    <content type="text"><![CDATA[简介过滤器是zuul最重要的组件，几乎它所有功能都是通过过滤器来实现的。因此，理解各个过滤器的功能是阅读源码必不可少的环节。 过滤器的分类按功能分类如果按照功能分类，过滤器主要有四大类：pre/route/post/error，它们之间的逻辑关系在ZuulServlet中有描述。除此之外你也可以自定义特殊类型的过滤器，比如源码中就有一个healthcheck类型。不过自定义类型的过滤器不会被zuul自动识别，需要使用者手动触发调用。 ZuulFilterZuulFilter是所有过滤器的基类，其核心方法是runFilter，应用了模板方法模式，来看下代码 // ZuulFilter.java public ZuulFilterResult runFilter() { ZuulFilterResult zr = new ZuulFilterResult(); // 当前filter是否被禁用 if (!isFilterDisabled()) { // 是否满足filter执行条件 if (shouldFilter()) { Tracer t = TracerFactory.instance().startMicroTracer("ZUUL::" + this.getClass().getSimpleName()); try { // 具体的filter逻辑执行的地方 Object res = run(); // wrap一下结果 zr = new ZuulFilterResult(res, ExecutionStatus.SUCCESS); } catch (Throwable e) { t.setName("ZUUL::" + this.getClass().getSimpleName() + " failed"); zr = new ZuulFilterResult(ExecutionStatus.FAILED); zr.setException(e); } finally { t.stopAndLog(); } } else { zr = new ZuulFilterResult(ExecutionStatus.SKIPPED); } } } 上面代码中的关键方法有两个：shouldFilter()和run()。 这两个方法都是抽象的，需要由具体的过滤器去实现。一般来说，我们在阅读过滤器源码时只需要重点关注这两个方法即可。 zuul-netflix-webapp提供的过滤器zuul-netflix-webapp模块中提供了一些有用的过滤器，在src/main/groovy/filters目录下 名称 类别 Order DebugFilter pre 1 Routing pre 1 PreDecoration pre 20 WeightedLoadBalancer pre 30 DebugRequest pre 10000 ZuulNFRequest route 10 ZuulHostRequest route 100 Postfilter post 10 RequestEventInfoCollectorFilter post 99 sendResponse post 1000 Stats post 2000 ErrorResponse error 1 Options static 0 Healthcheck healthcheck 0 由于篇幅关系，详细的代码分析拆分为以下三个章节： Pre和其它类型过滤器 Route类型过滤器 Post类型过滤器 zuul-simple-webappzuul-simple-webapp提供的过滤器较为简单，实现的功能基本上就是zuul-netfilx-webapp功能的一个子集，因此这里就不列出来了。]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— zuul整体架构]]></title>
    <url>%2Fsource%2Fzuul%2Farchitecture.html</url>
    <content type="text"><![CDATA[工作原理关于zuul架构，官方的How-it-Works已经讲得很清楚了，这里简单地翻译一下： Zuul的核心就是一系列的过滤器，能够在HTTP请求和响应的过程中进行一系列的操作。Zuul提供了一个可以在运行时动态地读取、编译并运行这些过滤器的框架。以下是过滤器的一些重要属性： 类型：通常定义了过滤器会作用在路由的哪个阶段 执行顺序：当同一阶段存在多个过滤器时，决定了这些过滤器的执行顺序 执行条件：决定过滤器是否被执行 行为：当符合条件的时候执行的操作 过滤器之间不会直接进行通信，而是通过每个请求唯一对应的RequestContext实例进行状态共享。过滤器目前只能通过Groovy语言编写（指的是动态过滤器），尽管从理论上来说Zuul支持所有以JVM为执行环境的语言。过滤器的源码应该写到指定的目录中，Zuul Server会周期性地检查这些目录的变化。一旦某些过滤器有更新，它们将会被Zuul读取并动态地编译到运行时环境中，在随后到来的每个请求中都将被Zuul调用。 图片来源 - netflix techblog 核心类ZuulServlet&lt;!-- web.xml --> &lt;servlet> &lt;servlet-name>ZuulServlet&lt;/servlet-name> &lt;servlet-class>com.netflix.zuul.http.ZuulServlet&lt;/servlet-class> &lt;/servlet> &lt;servlet-mapping> &lt;servlet-name>ZuulServlet&lt;/servlet-name> &lt;url-pattern>/*&lt;/url-pattern> &lt;/servlet-mapping> Zuul是一个web应用，在web.xml中其使用的Servlet实现是ZuulServlet。因此毫无疑问的，ZuulServlet就是整个Zuul的核心。 来看看servier方法 // ZuulServlet.java @Override public void service(javax.servlet.ServletRequest servletRequest, javax.servlet.ServletResponse servletResponse) throws ServletException, IOException { try { // 初始化当前的zuul request context，将request和response放入上下文中 init((HttpServletRequest) servletRequest, (HttpServletResponse) servletResponse); // Marks this request as having passed through the "Zuul engine", as opposed to servlets // explicitly bound in web.xml, for which requests will not have the same data attached RequestContext context = RequestContext.getCurrentContext(); // 为此次请求设置标识 context.setZuulEngineRan(); // zuul对请求的处理流程 start // 以下几个try块部分是zuul对一个请求的处理流程：pre -> route -> post // 可以看到： // 1. post是必然执行的（可以类比finally块），但如果在post中抛出了异常，交由error处理完后就结束，避免无限循环 // 2. 任何阶段抛出了ZuulException，都会交由error处理 // 3. 非ZuulException会被封装后交给error处理 try { preRoute(); } catch (ZuulException e) { error(e); postRoute(); return; } try { route(); } catch (ZuulException e) { error(e); postRoute(); return; } try { postRoute(); } catch (ZuulException e) { error(e); return; } // zuul对请求的处理流程 end } catch (Throwable e) { error(new ZuulException(e, 500, "UNHANDLED_EXCEPTION_" + e.getClass().getName())); } finally { // 此次请求完成，移除相应的上下文对象 RequestContext.getCurrentContext().unset(); } } 上面代码逻辑正好zuul官方的架构图相对应 RequestContextRequestContext是各filter之间进行消息传递的介质，其本质上是一个Map，这样就可以在上下文中存储任何kv pair。 TODO:: 这里有一个疑问，为什么RequestContext是继承自ConcurrentHashMap？因为理论上来说RequestContext对象是绝对线程安全的（线程隔离）。 RequestContext包含一个类变量threadLocal， protected static final ThreadLocal&lt;? extends RequestContext> threadLocal = new ThreadLocal&lt;RequestContext>() { @Override protected RequestContext initialValue() { try { // 上下文实例类型取决于contextClass，例如在NFRequestContext中就改写了contextClass return contextClass.newInstance(); } catch (Throwable e) { throw new RuntimeException(e); } } } 在代码的任何地方都可以通过静态方法getCurrentContext获取到属于当前线程（实际上经过zuul的处理，context实例是按请求隔离的）的RequestContext实例 public static RequestContext getCurrentContext() { if (testContext != null) return testContext; RequestContext context = threadLocal.get(); return context; } NFRequestContextNFRequestContext继承自RequestContext，定义了一些与netflix其它组件有关的特定概念和数据的key，例如eureka VIP，Ribbon client返回的repsonse等。它有一个静态方法 // NFRequestContext.java static { RequestContext.setContextClass(NFRequestContext.class); } 也就是说，只要加载了类NFRequestContext，应用中所有RequestContext的实例都将是NFRequestContext类型。 ZuulRunnerZuulServlet并不做任何实际的操作，而是将所有操作交给ZuulRunner完成。 而事实上ZuulRunner的大部分操作也是委托给FilterProcessor去完成的，除了init方法。 // ZuulRunner.java /** * sets HttpServlet request and HttpResponse */ public void init(HttpServletRequest servletRequest, HttpServletResponse servletResponse) { RequestContext ctx = RequestContext.getCurrentContext(); if (bufferRequests) { // wrap request并缓存其body // 所谓buffer是指将其内容缓存起来，使得可以安全地重复调用getReader(), getInputStream()等方法 // 因为一般来说，流操作一次之后就不能重复操作了 // 类com.netflix.zuul.http.HttpServletRequestWrapper注释上有详解 ctx.setRequest(new HttpServletRequestWrapper(servletRequest)); } else { ctx.setRequest(servletRequest); } // response没得选，肯定是wrap过的 ctx.setResponse(new HttpServletResponseWrapper(servletResponse)); } FilterProcessorFilterProcessor是个单例，通过getInstance()方法获取。 FilterProcessor的核心方法有两个：runFilters和processZuulFilter // FilterProcessor.java /** * 这个是真正执行filter的方法，每调用一次都会执行同一类型的所有filter * runs all filters of the filterType sType/ Use this method within filters to run custom filters by type * * @param sType the filterType. * @throws Throwable throws up an arbitrary exception */ public Object runFilters(String sType) throws Throwable { // 添加debug信息 if (RequestContext.getCurrentContext().debugRouting()) { Debug.addRoutingDebug("Invoking {" + sType + "} type filters"); } boolean bResult = false; // 通过FilterLoader获取指定类型的所有filter List&lt;ZuulFilter> list = FilterLoader.getInstance().getFiltersByType(sType); if (list != null) { // 这里没有进行try...catch... 意味着只要任何一个filter执行失败了整个过程就会中断掉 for (int i = 0; i &lt; list.size(); i++) { ZuulFilter zuulFilter = list.get(i); Object result = processZuulFilter(zuulFilter); if (result != null &amp;&amp; result instanceof Boolean) { // 注意这里写的是|=不是!= // TODO:: 为什么要用这种写法？既然只有result为boolean类型时才执行，直接赋值不行吗 bResult |= ((Boolean) result); } } } return bResult; } // FilterProcessor.java /** * 执行单个zuul filter * Processes an individual ZuulFilter. This method adds Debug information. Any uncaught Thowables are caught by this method and converted to a ZuulException with a 500 status code. * * @param filter * @return the return value for that filter * @throws ZuulException */ public Object processZuulFilter(ZuulFilter filter) throws ZuulException { RequestContext ctx = RequestContext.getCurrentContext(); boolean bDebug = ctx.debugRouting(); final String metricPrefix = "zuul.filter-"; // 这个变量没有用到... long execTime = 0; String filterName = ""; try { long ltime = System.currentTimeMillis(); filterName = filter.getClass().getSimpleName(); RequestContext copy = null; Object o = null; Throwable t = null; if (bDebug) { Debug.addRoutingDebug("Filter " + filter.filterType() + " " + filter.filterOrder() + " " + filterName); // copy了一份context用于debug copy = ctx.copy(); } ZuulFilterResult result = filter.runFilter(); ExecutionStatus s = result.getStatus(); // 统计执行时间 execTime = System.currentTimeMillis() - ltime; // 这段对过滤器的执行状态进行记录 switch (s) { case FAILED: t = result.getException(); ctx.addFilterExecutionSummary(filterName, ExecutionStatus.FAILED.name(), execTime); break; case SUCCESS: o = result.getResult(); ctx.addFilterExecutionSummary(filterName, ExecutionStatus.SUCCESS.name(), execTime); if (bDebug) { Debug.addRoutingDebug("Filter {" + filterName + " TYPE:" + filter.filterType() + " ORDER:" + filter.filterOrder() + "} Execution time = " + execTime + "ms"); Debug.compareContextState(filterName, copy); } break; default: break; } if (t != null) throw t; // 触发一个filter usage回调 // 当前notifier的实现固定是BasicFilterUsageNotifier，通过Servo统计filter的调用 usageNotifier.notify(filter, s); return o; } catch (Throwable e) { if (bDebug) { Debug.addRoutingDebug("Running Filter failed " + filterName + " type:" + filter.filterType() + " order:" + filter.filterOrder() + " " + e.getMessage()); } usageNotifier.notify(filter, ExecutionStatus.FAILED); if (e instanceof ZuulException) { throw (ZuulException) e; } else { ZuulException ex = new ZuulException(e, "Filter threw Exception", 500, filter.filterType() + ":" + filterName); // 如果在line35之前抛出了异常，这个execTime的值会是0 // 不过ZuulFilter.runFilter()中做了try...catch...处理，理论上来说不会出现异常 ctx.addFilterExecutionSummary(filterName, ExecutionStatus.FAILED.name(), execTime); throw ex; } } } // FilterProcessor.java /** * Publishes a counter metric for each filter on each use. */ public static class BasicFilterUsageNotifier implements FilterUsageNotifier { private static final String METRIC_PREFIX = "zuul.filter-"; @Override public void notify(ZuulFilter filter, ExecutionStatus status) { // 通过Netflix Servo对每个filter进行调用计数 DynamicCounter.increment(METRIC_PREFIX + filter.getClass().getSimpleName(), "status", status.name(), "filtertype", filter.filterType()); } } StartServerStartServer是一个ServletContextListener，负责在web应用启动后执行一些初始化操作 // zuul-netflix-webapp // StartServer.java protected void initialize() throws Exception { // 这个操作是触发静态变量AmazonInfoHolder.INFO的初始化，并不是没有意义的 AmazonInfoHolder.getInfo(); // 监控、度量等初始化 initPlugins(); // 动态Filter等相关类的初始化 initZuul(); // cassandra初始化 initCassandra(); // NIWS: Netflix Internal Web Service // 主要是初始化ribbon的客户端之类的 initNIWS(); // 初始化完成，修改当前应用实例的状态为up ApplicationInfoManager.getInstance().setInstanceStatus(InstanceInfo.InstanceStatus.UP); } 相关链接 FilterLoader工作方式]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析 —— 搭建调试环境]]></title>
    <url>%2Fsource%2Fzuul%2Fdebug_env.html</url>
    <content type="text"><![CDATA[下载源码 netflix-zuul 建议fork一份，方便阅读时随手写些注释提交。 zuul模块介绍 zuul-core: 确立zuul整体架构及重要类的api zuul-netflix: 提供了对netflix其它组件的集成相关实现，如hystrix, ribbon等。此外还有一些工具，如统计工具。主要是为zuul-netflix-webapp提供支持 zuul-netflix-webapp: Netflix提供的一个可用于生产环境的应用实现，包括Filter的具体实现，监控，动态Filter管理等功能 zuul-simple-webapp: 一个简单的示例app，功能比较简单，没有太大的研究价值 运行zuul-simple-webapp直接运行以下命令即可 $ cd zuul-simple-webapp $ ../gradlew jettyRun 然后可以通过 http://localhost:8080 访问httpbin。 详细可以参考zuul-simple-webapp 什么是httpbin A simple HTTP Request &amp; Response Service 简单来说就是一个方便测试HTTP请求和响应的各种信息,比如cookie, ip, headers和登录验证等的工具。 运行zuul-netflix-webapp由于这是一个可用于生产环境的应用，在其中集成了eureka等，因此需要运行起来还需要做一定的配置 配置zuul.properties，位于zuul-netflix-webapp/src/resources，这里只列出一些必须要覆盖的项 # zuul.properties eureka.serviceUrl.default={你的eureka server注册地址} # 指定filter存放的目录，由于只是调试代码，直接使用zuul提供的filter就好 zuul.filter.pre.path=src/main/groovy/filters/pre zuul.filter.routing.path=src/main/groovy/filters/route zuul.filter.post.path=src/main/groovy/filters/post # origin client对应的eureka VIP origin.zuul.client.DeploymentContextBasedVipAddresses=foo origin.zuul.client.Port=8080 此外，建议修改代码打开debug开关 // Debug.groovy boolean shouldFilter() { return true; } 然后运行 $ cd zuul-netflix-webapp $ ../gradlew jettyRun 不出意外的话，此时你的zuul应用应该已经注册到eureka server中了。访问 http://localhost:8080/healthcheck ，如果出现ok，说明应用已运行成功访问 http://localhost:8080/path 可以路由到你的foo服务的path路径中 debug运行(IntelliJ IDEA)上面的操作可以运行zuul的webapp，但是还没办法进入断点调试，接下来我们尝试配置用IntelliJ IDEA来进行断点调试。 添加gradle run/debug配置在IDEA的run/debug configurations添加一个Gradle配置，参数如下 Gradle project: zuul-simple-webapp或zuul-netflix-webapp Tasks: jettyRun VM Options: -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=7777 然后运行，这时应用会进入无限等待remote连接的状态，因此需要再添加一个remote配置。 添加remote run/debug配置在IDEA的run/debug configrations添加一个Remote配置，参数如下 Host: localhost Port: 7777（即之前Gradle配置中VM Options的address） 然后debug运行，就可以愉快地进行调试了。]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul源码解析目录]]></title>
    <url>%2Fsource%2Fzuul%2Fhome.html</url>
    <content type="text"><![CDATA[前言最近看了芋道的eureka源码解析。上面还提到了一些阅读源码的方法，觉得挺有意思的，就想自己尝试一下。正好zuul相对完整系统的源码解析文章在网上还没有，而且zuul的源码相对来说还是比较简单的，所以第一篇源码解析文章就选择了zuul。 此系列文章基于zuul 1.3.0。 此系列文章与spring cloud zuul无关，是对原生netflix zuul的源码解析。对于spring cloud zuul的源码解析，网上相关的文章还是比较多的。 在Zuul中，过滤器分为容器（jetty）的过滤器（Filter）和Zuul的过滤器（ZuulFilter）。由于在我们关注的更多的是Zuul的过滤器，因此在文章中如无特殊说明，提到过滤器均指Zuul的过滤器。 文章目录 zuul源码解析 —— 搭建调试环境 zuul源码解析 —— zuul整体架构 zuul源码解析 —— 过滤器 zuul源码解析 —— Pre和其它类型的过滤器 zuul源码解析 —— Route类型过滤器 zuul源码解析 —— Post类型过滤器 zuul源码解析 —— 动态加载Filter（未完成） TODO LIST个人记录的todo list，请无视 monitor, tracer和counter - 都是netflix servo的东西 filter registry groovy filter manager: FilterFileManager, FilterLoader filter loader filter usage notifier groovy的filter如何执行？ zuul的配置与ServletConfig zuul servlet, zuul runner, filter processor httpbin 贯串整个请求的RequestContext pre, route, post, error zuul的request wrapper有何不同？ zuul对一次请求的处理流程 Debug类 zuul的动态配置（DynamicPropertyFactory） - 属于zuul archaius的内容 zuul event vip, routevip, altvip都是些啥 RequestContext与NFRequextContext到底用哪个，是在哪里决定的？ SurgicalDebugFilter WeightedLoadBalancer]]></content>
      <categories>
        <category>source</category>
        <category>zuul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[UNIX基础知识]]></title>
    <url>%2FUNIX_basic_knowledge.html</url>
    <content type="text"><![CDATA[UNIX基础知识操作系统简介所有的操作系统都为它们所运行的程序提供服务，包括：执行新程序、打开文件、读写文件、分配存储区等。 严格意义上说，操作系统也是一种软件，它控制计算机硬件资源，提供程序运行环境。通常将这种软件称为内核。内核的接口被称为系统调用，公用函数库建立在系统调用接口之上，shell则是一种特殊的应用程序，为其它应用程序提供接口。 Linux是GNU操作系统使用的内核。 每一个进程都有一个工作目录，所有的相对路径都从工作目录开始解释。 文件描述符文件描述符是内核用以标识一个特定进程正在访问的文件，进程读写文件时使用。 每运行一个新程序时，所有的shell都为其打开3个文件描述符：标准输入0、标准输出1、标准错误2。如果不做特殊处理，则所这3个描述符都将链接向终端。大多数shell都提供将这3个描述符重定向的方法，如bash $ ls > out.txt 参考 关于文件描述符 程序程序是一个存储在磁盘上某个目录中的可执行文件。内核使用exec函数将程序读入内存并执行。 程序的执行实例称为进程。每个进程都有一个唯一的数字标符，称为进程ID。进程可以在前台运行，将输出显示在屏幕上，也可以在后台运行。内核控制着系统如何管理运行在系统上的所有进程。 信号信号用于通知进程发生了某种情况。进程接收到信号后会进行相应的处理。 很多情况都会产生信号。如在终端键盘上可以通过中断键（Ctrl-C）和退出键（Ctrl-\）产生相应的信息，用于中断当前运行的进程。如在另一个进程中可通过kill函数向另一个进程发送一个信号中断其运行。 时间日历时间其值为自协调世界时（UTC）。 进程时间也称CPU时间，用以度量进程使用的中央处理器资源。有3个进程时间值： 时钟时间：进程运行的时间总量 用户CPU时间：执行用户指令所用的时间量 系统CPU时间：为该进程执行内核程序所经历的时间 系统调用和库函数所有的操作系统都提供多种服务的入口点，由此程序向内核请求服务。这些入口点即称为系统调用。 系统调用的接口是用c语言定义的，与具体系统如何执行该系统调用的实现技术无关。 通用库函数可能会调用一个或多个内核的系统调用，也可能不会，但它们并不是内核的入口点。从实现来看，库函数与系统调用之间有着本质区别。但从用户角度看，这区别并不重要——系统调用和库函数都是以c函数的形式出现。 系统内存管理内核通过硬盘上的存储空间来实现虚拟内存，这块区域称为交换空间。内核不断地在交换空间和实际的物理内存之间反复交换虚拟内容中的内容，使得系统以为它拥有比物理内存更多的可用内存。 UNIX标准及实现UNIX标准 ISO C IEEE POSIX SUS(Signle UNIX Spcification) UNIX系统的实现 SVR4 FreeBSD Linux Max OS X Solaris 其中，虽然只有Max OS X和Solaris能够称之为是一种UNIX系统，但它们都实现了UNIX标准并提供了UNIX的编程环境。]]></content>
      <categories>
        <category>UNIX</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《摄影师的视界》阅读笔记]]></title>
    <url>%2F%E3%80%8A%E6%91%84%E5%BD%B1%E5%B8%88%E7%9A%84%E8%A7%86%E7%95%8C%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[画框图像的背景即是画框。 数码摄影之前，最常见的画面区域是3:2的水平画框。也是最广泛使用的相机格式。现在胶片的物理宽度不再受限制， 大多数中低端的相机采用了更窄更自然的4:3格式。 根据被摄主体和摄影师所选的处理方式的不同，画框边界对影像的影响可以很强烈，也可以很微弱。 对角线在画面中与边界画框相对照，能产生强烈的对角张力，使图像更具动态。 构图时需要考虑画框中有几个主体，如果有多个主体，就要进行平衡，使得主要的主体更突出。 主体大小如何为主体分配在照片中占据的空间比例： 如果主体很不寻常、很有趣，往往其细节比较重要，应该让其占据更大的比例 反之就应该后退一些使我们能看到一些周围的环境 主体位置任何一张只有一个主体的照片里，除非采用将画框填满的构图方式，否则总需要决定怎么安排主体的位置与四周留白的比例。一旦决定在四周保留空间，主体的位置就是一个关键。 避免中央构图：缺乏想象力，过于单调乏味。绝对的中心点太过稳定，使画面缺乏动感张力。略微偏离中心会使主体更加融入环境。 如何需要采用不同寻常的构图，那么需要有一个合适的理由，否则整个布局就显得很荒谬和反常。 分割画框任何类型的任何影像都会自动分割画框（甚至平淡背景上的一个细小物体）。 如果从两个方向分割画框，就会产生一个交叉点。这通常是一个安排视觉重点或其他注意点的好位置。 地平线地平线对画框进行横向分割。 如果地平线是唯一有意义的元素，那它的位置就非常重要。 如果其它元素富有趣味性，也可以让其占据大部分画面，而不需要太过于在意地平线的位置。 框中框设计基础构图的本质是组织，使画框里的的有图像元素有序化。设计重要的是要理解原理，即与各人的欣赏口味没有关系，并可以解释为什么某些照片能给人留下深刻印象，某些影像组织方式会有某种可预计的效果。 构图类似语言：画框是语境，设计基础是语法，图像元素是词汇，元素处理是句法。 最基本的两种原理：对比和平衡。 设计还必须接受限制：观众对摄影的了解。许多观众也许对设计手法一窍不通，但是看过大量照片后也会了解一些惯例。所以，某些构图方法可以被看作是常规的，摄影师可以根据自己的意图遵从或挑战它们。 对比对比强调的是照片中图像元素之间的区别（影调、色彩、形态等），两个相互对比的元素能相互加强。 平衡平衡是隐含在对比之中的关系，是对立元素之间的主动关系。平衡是张力的结果，是对立的影响力相互匹配以提供均衡和协调的感觉。如果失去平衡，就会带来视觉张力。 视觉的基本原理是眼睛总会设法寻找某种张力的平衡力。因此，平衡是和谐，是结果，是直观感觉到美学愉悦的状况。 无论我们在谈论的是影调、色彩、点布局还是其它什么东西时，目标都是在寻找视觉的『重心』。 平衡类型 静态（对称）平衡：每样东西到照片中心的距离都相等，较为严肃、沉闷 动态平衡：使用不相等的重量和张力达成平衡，更加生动、活泼 对称平衡的张力被安排在居中的位置。动态平衡处理的则是不相等的重量和张力，这能使影像更加生动活泼。 tip 相比如何进行平衡，先探讨是否需要平衡可能更重要。 动态张力动态张力是使用各种结构内在的能力，以保持眼睛的警觉并使之从画面中心向外围移动。这与正规构图的静态特性正好相反。 取得动态张力的技术相当直接：一组向不同方向的斜线、相反的线条、任何引导眼睛走出画面的结构手段（最好是相反方向的）。 节奏当场景中出现一些相似的元素时，通过对它们的布局可以构成有节奏的视觉结构。 眼睛和大脑天生熟练于扩展所见的，会很乐意假设节奏的延续性。因此，感知到的重复影像会比实际看到的更长。 打破节奏的异常事物可以使影响更具动感。最好把该事物放在右边（因为眼睛通常从左往右跟踪节奏化结构，这样让眼睛有足够的时间建立节奏）。 透视与纵深『恒常比例』现象：两条平等线从我们身边延伸，会聚在远方，但同时我们却能感觉到它们是笔直而平等的。 纵深感对照片来说总是很重要。纵深感可以进一步影响照片的写实性。 摄影必须采用各种策略来加强或减弱画面纵深感。而影像有其自身的参照系，而不是正常的感知系。 线形透视线形透视的特点是会聚的直线，而这些直线在大多数场景里其实的平等线。 如果相机是水平的，而场景是风光，那么水平线将会聚到地平线。如果相机向上指，建筑边缘这样的垂直线将会聚在天空中的某个点。 收缩透视收缩透视主要出现在位于不同距离的很多相同或相似的物体。 空间透视大气灰雾降低了场景远处部分的反差，提亮影调。我们的眼睛会把这作为一个纵深的提示。 影调透视黑色背景前的明亮物体通常给人感觉很突出，因此有很强烈的纵深感。可以通过安排主体和光线来达到强化纵深的效果。 色彩透视暖色调具有前倾的感觉，而冷色调则退后。 清晰度清晰度高往往暗示着近距离，可以通过对焦来使画面的清晰度不同，从而达到强调纵深的效果。 内容的强与弱有些照片本身的内容就非常充实，过分地考虑构图可能适得其反。 图形与摄影元素所有形状中，隐隐约约构成的形状很储蓄、不惹眼，是最有用的，能帮助将一幅影像归整成可识别的形态，并通过简单的视觉效果引导眼睛满意地发现它们。 点单点单点占据画面非常小，为了醒目，它必须与环境存在对比。 多点一旦加入哪怕一个点，点之间的距离也会给画面带来距离感。 画面上的两个点会使视线来回地从一点移向另一点，导致两点之间产生隐含的连接线。 画面上的多个点则会隐含地让人感觉占据了它们之间的空间，导致它们组成了区域。 当画面上有非常多的点需要安排时，可以尝试自然随意的构图而非人为安排。 线大多数线条实际上是边界。 对比在定义视觉线条时扮演了重要的角色。 水平线垂直线斜线曲线视线形状三角形三角形是摄影构图中最有用的形状，部分因为它们易于构造或暗示，部分因为会聚效果。隐含三角形能给影像带来秩序，需要这种安排的地方通常是那些需要条理性的地方。 三角形天生是一种强烈的形状，非常吸引眼睛。而且，通常只要有两条线就够了，第三条可以假设或者使用合适的画框边界。 画框内构成的三角形应尽量大，充满整个画面，能使得照片看起来具有稳定感。 倒三角则相反，它显得不稳定，更具挑衅性。 用光线与色彩构图明暗与基调大多数照片的信息内容主要位于中间影调。然而，阴影和高光能对照片的情绪的气氛产生很大的影响。 如果忽略色彩，影调的分布由对比和亮度决定。把这两个因素当做指南，影像的风格选择基于三个方面：场景的特征、场景的照明方式、摄影师的诠释。 构图中的色彩 色相：色相是各种颜色之所以得名的特性 饱和度：饱和度是色相的密度或纯度 明亮度：明亮度决定了色相是黑暗的还是明亮的 强烈的色相以多层次的方式被感知，除了光学现实有关之外，也与文化和经历相关联。不同的颜色有不同的象征意义，应根据要拍摄的场景和主题来选择使用合适的颜色。 三原色 红色被认为是最强烈、密度最高的颜色，具有前进的倾向，可以强化纵深感。它充满活力、生机、强烈、温暖、炎热，暗示激情、侵略和危险。 黄色是所有颜色中最明亮的，甚至没有黑暗的形式。它精力充沛、锐利、坚定、好斗、欢乐，能与太阳以及其他光源相关联。 蓝色比黄色收敛，倾向于安静，相对较暗，显得冷静。蓝色较透明，有很多形式，很多人难以精确判断。 三补色 绿色通常是正面的，象征生长、希望和进步。绿色的负面关联则是疾病和腐烂。 紫色是难以捉摸的、稀有的颜色。难以寻找、捕捉，也难以精确再现。紫色是富有、奢侈、神秘、浩瀚的象征。 橙色温暖、强烈、辉煌、有力，也代表炎热和干燥。 色彩关系色彩必须按彼此之间的关系来处理。视觉相邻的色彩不同，它们的感受也不同。 色彩组合两种和谐关系： 互补色和谐 相似色和谐 不同的色彩被人们感受到的亮度值是不同的。例如，黄色最亮，紫色最暗。 色彩重点一小块对比色具有聚焦的效果。最明显的效果是当环境是相对水色的，而纯色相占据了局部区域。这种特别形式的色彩对比无可避免地会显得更为突出，称之为地方色彩。 温和的色彩除非在人造的环境，强烈的色彩在自然世界上相对稀少。 温和色彩更细微，提供更安静甚至更优雅的愉悦。 黑白黑和白更能全面地表现影调变化、质感再现、形式模型和定义形状。]]></content>
      <categories>
        <category>摄影</category>
        <category>构图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redisson学习笔记]]></title>
    <url>%2Fredisson%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[认识redisson来看下官方的介绍： Redis based In-Memory Data Grid for Java. State of the Art Redis client. 可以知道，redisson是Java的一款redis客户端。 作为redis客户端，它和大名鼎鼎的jedis有什么区别呢？redisson的宗旨是促进使用者对redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 换句话说，redisson对redis的操作进行了一些更高级的抽象，使得我们能够轻松地实现一些复杂的功能，如一系列的Java分布式对象，常用的分布式服务等。而作为抽象的代价，就是丢失了对底层细节的掌控。 Getting Startredisson官方就支持与spring-boot集成，因此根据官方文档直接依赖 &lt;dependency> &lt;groupId>org.redisson&lt;/groupId> &lt;artifactId>redisson-spring-boot-starter&lt;/artifactId> &lt;version>3.8.0&lt;/version> &lt;/dependency> 再配置相关yaml，以最简单的本地单机模式启动 application.yaml spring: redis: redisson: config: classpath:redisson.yaml redisson.yaml singleServerConfig: address: redis://127.0.0.1:6379 然而添加依赖后直接启动spring boot应用居然报错了，注入RedissonClient失败！ 妈耶，Google了一下无果，直接看源码，居然发下redisson-spring-boot-starter这个包没有spring.factories文件，也即是说RedissonAutoConfiguration不会自动加载。。 于是补上 @SpringBootApplication @ImportAutoConfiguration(RedissonAutoConfiguration.class) public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 虽然不知道redisson官方出于什么原因没有提供spring.factories文件，总之再次启动，正常。 redisson锁实践接口调用限制redisson执行lua脚本redisson提供了很方便地执行lua脚本的方式 redissonClient.getScript().eval( RScript.Mode.READ_ONLY, //执行模式 "return redis.call('get', KEYS[1])", // 要执行的lua脚本 RScript.ReturnType.INTEGER, // 返回值类型 Lists.newArrayList("tac"), // 传入KEYS true, 1L, "hello" //传入ARGV ) 更具体可以参考官方文档脚本执行 踩过的一些坑传入的Boolean值参数会变成字符串假设通过redisson的eval()传入的ARGV = false，那么在lua脚本中 print(type(ARGV[1])) --输出'string' 传入数值也会变成字符串，非int型则会被序列化存储假设通过redisson的eval()传入的ARGV = 1L，那么在lua脚本中获取会变成 print(ARGV[1]) --输出'["java.lang.Long",1]' 若传入的是ARGV = 1，则 print(ARGV[1]) --输出'1' 直接从redis.call()获取得值是int型，而在lua中进行了数值操作后得到的值却是long型例如，以下结果是转换为java.lang.Integer return redis.call('get', 'tac') 而以下结果却是转换为java.lang.Long local tac = redis.call('get', 'tac') return tac + 1 一些小细节 注意是redisson，不要写成redission redisson提供的所有类都是以R开头的，如RLock]]></content>
      <categories>
        <category>java</category>
        <category>redisson</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zuul中hystrix默认timeout配置失效的原因]]></title>
    <url>%2Fzuul_hystrix_default_timeout_config_invalid_reason_research.html</url>
    <content type="text"><![CDATA[所处环境 spring boot: 1.5.9.RELEASE spring cloud: Edgware.RELEASE 问题描述根据netflix hystrix官方的描述，可以通过hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds这个全局配置为所有服务配置默认超时时间。然而在实践中却发现该配置并没有生效（一直是2000ms），但针对各服务进行单独配置却是可以生效的。 问题定位经google发现，hystrix的timeout机制是通过HystrixTimer来处理的。 HystrixTimer是一个单例，开发人员可以在执行HystrixCommand前通过调用HystrixTimer.getInstance().addTimerListener()方法来添加一个定时的listener，然后在command on completed的时候移除它。相关的代码已经由netflix实现了，可以查看源码AbstractCommand.java TimerListener listener = new TimerListener() { @Override public int getIntervalTimeInMilliseconds() { return originalCommand.properties.executionTimeoutInMilliseconds().get(); } }; final Reference&lt;TimerListener> tl = HystrixTimer.getInstance().addTimerListener(listener); // set externally so execute/queue can see this originalCommand.timeoutTimer.set(tl); /** * If this subscriber receives values it means the parent succeeded/completed */ Subscriber&lt;R> parent = new Subscriber&lt;R>() { @Override public void onCompleted() { if (isNotTimedOut()) { // stop timer and pass notification through tl.clear(); child.onCompleted(); } } @Override public void onError(Throwable e) { if (isNotTimedOut()) { // stop timer and pass notification through tl.clear(); child.onError(e); } } }; 从以上代码可以很容易追溯到hystrix超时时间是从originalCommand.properties(HystrixCommandProperties)这个对象中获取的，而在spring cloud提供的AbstractRibbonCommand中存在以下代码 final HystrixCommandProperties.Setter setter = HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(zuulProperties.getRibbonIsolationStrategy()).withExecutionTimeoutInMilliseconds( RibbonClientConfiguration.DEFAULT_CONNECT_TIMEOUT + RibbonClientConfiguration.DEFAULT_READ_TIMEOUT); 导致originalCommand.properties在构建时可以获取到一个executionTimeoutInMilliseconds的实例级默认值，从而覆盖掉了全局的executionTimeoutInMilliseconds配置，但实例级的配置并不受影响，因为hystrix property优先级为： Global default from code Dynamic global default property Instance default from code Dynamic instance property 解决方案 改写HttpClientRibbonCommand的实现//todo:: 待补充 升级spring cloud版本到Edgware.SR1以上Spring cloud在Edgware.SR1版本中已经解决了此问题(issue2633)，如果有条件可以直接通过升级版本来解决此问题。 参考链接 Hystrix Configurations 聊聊hystrix的timeout处理 - segmentfault Hystrix工作原理 ribbon设置url级别的超时时间]]></content>
      <categories>
        <category>java</category>
        <category>踩坑日记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[声明式http客户端openfeign的研究与应用]]></title>
    <url>%2Fopenfeign_research_and_application.html</url>
    <content type="text"><![CDATA[简介openfeignopenfeign，简称feign，是netflix开源的技术栈之一。feign的主旨是使得编写java http客户端更容易。为了贯彻这个理念，feign采用了通过处理注解来自动生成请求的方式（官方称呼为声明式、模板化）。因此，基于feign编写的http客户端画风看起来是这样的 interface Bank { @RequestLine("POST /account/{id}") Account getAccountInfo(@Param("id") String id); } 然后通过一系列操作可以为Bank接口在运行时自动生成对应的实现。通过这个实现我们就可以在java中像调用一个本地方法一样完成一次http请求，大大减少了编码成本，同时提高了代码可读性。 spring-cloud-openfeignspring-cloud-feign基于自动配置功能（autoconfiguration），为spring boot应用提供openfiegn的集成： 支持通过JAX-RS或Spring MVC annotations来构建feign client 自动使用Spring MVC的HttpMessageConverters来完成序列化反序列化 集成了ribbon和hystrix，只要在项目中引入相关的依赖即可立即使用 无需任何配置，开箱即用，秉承了spring-boot一贯的作风。 官方文档参考 常见问题及解决方案处理响应数据的通用部分api响应数据格式，一般而言除实际的业务数据外是固定的格式，其中包括了对此次请求的处理信息描述。对于这部分数据，应由feign进行统一处理。 解决方案feignclient是通过Decoder来对请求响应进行处理的，因此可以使用自定义的Decoder来处理响应数据的通用部分。 例如可以定义一个UnwrapRestfulApiResponseSpringDecoder来对RestfulApiResponse进行自动拆包操作，并通过@FeignClient的configuration属性配置其使用的Decoder。 @FeignClient(name = AuthCenter.SERVICE_NAME, path = RMIPath.USER, configuration = TenantUserClient.Configuration.class) public interface TenantUserClient { class Configuration { @Autowired private ObjectFactory&lt;HttpMessageConverters> messageConverters; @Bean public Decoder feignDecoder() { return new ResponseEntityDecoder(new UnwrapRestfulApiResponseSpringDecoder(this.messageConverters)); } } } 本地调用需要注册到注册中心导致服务不可用的问题在实际开发过程中，有时会出现需要在本地调用远程服务来进行调试的情况，此时我们需要将本地的应用注册到注册中心。而一旦这样做，会导致该服务存在多个节点（一个远程一个本地），从而导致依赖该服务的应用软负载均衡负载到本地节点时会失败。 解决方案1. 不通过注册中心获取信息，使用直接指定url的方式调用（✘不推荐）参考代码 @FeignClient(name = "sysinfo-service" ,url = "http://49.4.7.72", path = "/") public interface SysinfoClient { } 此方案优点是简单易用，但缺点也很明显： 在应用实际上线时需要将调用方式修改为通过注册中心调用，否则client将无法进行负载均衡，因此需要频繁改动代码 在特定环境下，即使忘记切换调用方式有时也不影响client的使用，会将问题隐藏，埋下隐患 并没有与其它服务真正地解耦，一旦依赖的服务故障就会导致本地开发无法进行 由于存在上述缺点，现一般不推荐使用该方案。 2. 使用打桩的方式与远程服务解耦（✔推荐）大致思路为，通过Spring的@Profile注解，在不同的环境为应用注册不同的client bean（例如在本地环境使用Hard Code bean，而在非本地环境则使用远程调用的client bean）。 参考代码ClientConfiguration.java @EnableAuthClient @EnableFeignClients(basePackages = "com.cheegu.icm.biz.foo.client") @Configuration @Profile({SpringProfiles.DEV, SpringProfiles.TEST, SpringProfiles.PROD}) public class ClientConfiguration { } MockClientConfiguration.java @Configuration @Profile(SpringProfiles.LOCAL) public class MockClientConfiguration { @Bean public SysinfoClient sysinfoClient() { return new SysinfoClient() { @Override public UserInfoDto user() { return new UserInfoDto(); } @Override public TableData&lt;UserRowDto> users() { return TableData.empty(); } }; } @Bean public TenantUserClient tenantUserClient() { return new TenantUserClient() { @Override public List&lt;ResourceInfoDto> listMyAlResource() { return new ArrayList&lt;>(); } }; } } 如何在调用时带上请求头在使用客户端进行调用时，有时会需要带上某些请求头（例如认证token），但又不希望在代码中手动去做这些操作。 解决方案可以通过实现feign提供的RequestInterceptor接口来对请求进行切面处理。这样每次通过client发起请求时都会由feign interceptor进行拦截，为请求添加headers。 参考代码 @Configuration public class AppConfiguration implements EnvironmentAware { private Environment environment; @Bean public AuthorizationInfoForwardingInterceptor authorizationInfoForwardingInterceptor() { //这个interceptor会为请求自动带上认证token return new AuthorizationInfoForwardingInterceptor(environment.getProperty("spring.application.name")); } @Override public void setEnvironment(Environment environment) { this.environment = environment; } } 指定了get方法，但feign仍然使用了post方法进行请求如下代码所示 @FeignClient("microservice-provider-user") public interface UserFeignClient { @RequestMapping(value = "/get", method = RequestMethod.GET) public User get0(User user); } 通过@RequestMapping的method属性指定了请求方法为GET，但实际运行会发现feign仍然使用了POST方法进行了调用。 解决方案这种写法并不正确，正确写法有二： 1. 通过@RequestParam指定url参数名@FeignClient(name = "microservice-provider-user") public interface UserFeignClient { @RequestMapping(value = "/get", method = RequestMethod.GET) public User get1(@RequestParam("id") Long id, @RequestParam("username") String username); } 2. 当目标url参数较多时，可使用map来构建@FeignClient(name = "microservice-provider-user") public interface UserFeignClient { @RequestMapping(value = "/get", method = RequestMethod.GET) public User get2(@RequestParam Map&lt;String, Object> map); } 使用MultipartFile类型作为请求参数有时对外提供的接口调用可能需要传送文件，此时需要对MultipartFile类型的参数进行处理 解决方案待补充]]></content>
      <categories>
        <category>java</category>
        <category>spring cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[同时配置UrlRewriteFilter与ShiroFilter导致outbound rule失效的问题]]></title>
    <url>%2Foutbound_rule_invalid_cause_by_urlrewritefilter_and_shirofilter.html</url>
    <content type="text"><![CDATA[项目环境Spring Boot1.5.4 + Shiro1.4.0 + UrlRewriteFilter4.0.4 问题描述最近在项目中同时用到Shiro和UrlRewriteFilter。由于在web环境下两者都是通过配置filter来实现功能的，因此导致出现了冲突——UrlRewriteFilter的outbound rule无法正常工作。具体表现为： 在urlrewrite.xml中配置了 &lt;urlrewrite> &lt;outbound-rule> &lt;from>/in-rewrite\?type=(\w+)$&lt;/from> &lt;to>/in/$1.html&lt;/to> &lt;/outbound-rule> &lt;/urlrewrite> 在java代码中定义action @GetMapping("out-rewrite") @ResponseBody public String out(HttpServletRequest request, HttpServletResponse response) { return response.encodeURL("localhost:8080/in-rewrite?type=233"); //will be rewrite to 'localhost:8080/in/233.html' } 正常来说，访问这个api应该返回”localhost:8080/in/233.html”才对，但却返回了未转换前的url，即”localhost:8080/in-rewrite?type=233”。 问题分析在调试源码的时候发现，同时配置了ShiroFilter和UrlRewriteFilter时，response的类型是ShiroHttpServletResponse，此时outbound rule是不起作用的。而把ShiroFilter的配置去掉，只留下UrlRewriteFilter时，response的类型则是UrlRewriteWrappedResponse，而此时outbound rule是起作用的。显然，两者在filter里面都对response进行了重新包装，而ShiroFilter执行在后，导致前者的部分功能失效。 再查看上面两个response包装类的源码： UrlRewriteWrappedResponse.java ... public String encodeURL(String s) { RewrittenOutboundUrl rou = this.processPreEncodeURL(s); if(rou == null) { return super.encodeURL(s); } else { if(rou.isEncode()) { rou.setTarget(super.encodeURL(rou.getTarget())); } return this.processPostEncodeURL(rou.getTarget()).getTarget(); } } ... ShiroHttpServletResponse.java ... public String encodeURL(String url) { String absolute = toAbsolute(url); if (isEncodeable(absolute)) { // W3c spec clearly said if (url.equalsIgnoreCase("")) { url = absolute; } return toEncoded(url, request.getSession().getId()); } else { return url; } } ... 发现两者均对encodeUrl、encodeRedirectUrl等四个方法进行了改写。Shiro的包装类执行在后，导致UrlRewriteWrappedResponse的相关代码没有被执行。 解决方案重排Filter的执行顺序让UrlRewriteWrappedResponse的包装过程执行在后，避免encodeUrl方法被改写。 显然这不是一个好的办法，原因如下： 导致ShiroHttpServletResponse的方法被改写，可能引出其它未知问题，无异于拆东墙补西墙 导致ShiroFilter执行在前，可能导致一些访问的url还未被重写，这次访问就被Shiro拦截了 自己实现encodeUrl的逻辑让ShiroHttpServletResponse执行encode前先执行UrlRewriteWrappedResponse的encode代码。 改写ShiroHttpServletResponse，重写其encode逻辑 public class CustomShiroHttpServletResponse extends ShiroHttpServletResponse { private HttpServletResponse wrapped; public CustomShiroHttpServletResponse(HttpServletResponse wrapped, ServletContext context, ShiroHttpServletRequest request) { super(wrapped, context, request); this.wrapped = wrapped; } @Override public String encodeRedirectURL(String url) { return super.encodeRedirectURL(wrapped.encodeRedirectURL(url)); } @Override public String encodeRedirectUrl(String s) { return super.encodeRedirectUrl(wrapped.encodeRedirectUrl(s)); } @Override public String encodeURL(String url) { return super.encodeURL(wrapped.encodeURL(url)); } @Override public String encodeUrl(String s) { return super.encodeUrl(wrapped.encodeUrl(s)); } } 改写ShiroFilter，使用CustomShiroHttpServletResponse代替ShiroHttpServletResponse public class CustomSpringShiroFilter extends AbstractShiroFilter { protected CustomSpringShiroFilter(WebSecurityManager webSecurityManager, FilterChainResolver resolver) { //这段代码是copy了ShiroFilterFactoryBean$SpringShiroFilter的构造方法中的代码 super(); if (webSecurityManager == null) { throw new IllegalArgumentException("WebSecurityManager property cannot be null."); } setSecurityManager(webSecurityManager); if (resolver != null) { setFilterChainResolver(resolver); } } @Override protected ServletResponse wrapServletResponse(HttpServletResponse orig, ShiroHttpServletRequest request) { //使用CustomShiroHttpServletResponse代替原有的Response Wrapper return new CustomShiroHttpServletResponse(orig, getServletContext(), request); } } 然后改写ShiroFilterFactoryBean的createInstance方法，使用CustomSpringShiroFilter，代替原来的Filter public class CustomShiroFilterFactoryBean extends ShiroFilterFactoryBean { private static transient final Logger log = LoggerFactory.getLogger(ShiroFilterFactoryBean.class); @Override protected AbstractShiroFilter createInstance() throws Exception { //这部分代码与父类相同 log.debug("Creating Shiro Filter instance."); SecurityManager securityManager = getSecurityManager(); if (securityManager == null) { String msg = "SecurityManager property must be set."; throw new BeanInitializationException(msg); } if (!(securityManager instanceof WebSecurityManager)) { String msg = "The security manager does not implement the WebSecurityManager interface."; throw new BeanInitializationException(msg); } FilterChainManager manager = createFilterChainManager(); PathMatchingFilterChainResolver chainResolver = new PathMatchingFilterChainResolver(); chainResolver.setFilterChainManager(manager); //使用CustomSpringShiroFilter代替SpringShiroFilter return new CustomSpringShiroFilter((WebSecurityManager) securityManager, chainResolver); } } 重启项目，访问”/out-rewrite”，结果如下]]></content>
      <categories>
        <category>java</category>
        <category>踩坑日记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在@MybatisTest中使用通用Mapper出现InstantiationException的问题]]></title>
    <url>%2Finstantiationexception_on_mybatistest.html</url>
    <content type="text"><![CDATA[问题描述在Spring Boot环境中使用@MybatisTest注解针对Mapper写单元测试的时候，由于同时引入了通用Mapper，在单元测试中调用通用Mapper的方法进行CRUD时会出现InstantiationException异常 org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.builder.BuilderException: Error invoking SqlProvider method (tk.mybatis.mapper.provider.base.BaseSelectProvider.dynamicSQL). Cause: java.lang.InstantiationException: tk.mybatis.mapper.provider.base.BaseSelectProvider at org.mybatis.spring.MyBatisExceptionTranslator.translateExceptionIfPossible(MyBatisExceptionTranslator.java:77) at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:446) at com.sun.proxy.$Proxy89.selectOne(Unknown Source) at org.mybatis.spring.SqlSessionTemplate.selectOne(SqlSessionTemplate.java:166) at org.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:82) at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:59) at com.sun.proxy.$Proxy91.selectByPrimaryKey(Unknown Source) at com.ikentop.biz.provider.mapper.hh.ImageRecordMapperTest.testSimply(ImageRecordMapperTest.java:33) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75) at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86) at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:252) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:94) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61) at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:191) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) Caused by: org.apache.ibatis.builder.BuilderException: Error invoking SqlProvider method (tk.mybatis.mapper.provider.base.BaseSelectProvider.dynamicSQL). Cause: java.lang.InstantiationException: tk.mybatis.mapper.provider.base.BaseSelectProvider at org.apache.ibatis.builder.annotation.ProviderSqlSource.createSqlSource(ProviderSqlSource.java:103) at org.apache.ibatis.builder.annotation.ProviderSqlSource.getBoundSql(ProviderSqlSource.java:73) at org.apache.ibatis.mapping.MappedStatement.getBoundSql(MappedStatement.java:292) at org.apache.ibatis.executor.CachingExecutor.query(CachingExecutor.java:81) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:148) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:141) at org.apache.ibatis.session.defaults.DefaultSqlSession.selectOne(DefaultSqlSession.java:77) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:433) ... 34 more Caused by: java.lang.InstantiationException: tk.mybatis.mapper.provider.base.BaseSelectProvider at java.lang.Class.newInstance(Class.java:427) at org.apache.ibatis.builder.annotation.ProviderSqlSource.createSqlSource(ProviderSqlSource.java:85) ... 45 more Caused by: java.lang.NoSuchMethodException: tk.mybatis.mapper.provider.base.BaseSelectProvider.&lt;init>() at java.lang.Class.getConstructor0(Class.java:3082) at java.lang.Class.newInstance(Class.java:412) ... 46 more 单元测试代码如下： @RunWith(SpringRunner.class) @MybatisTest public class ImageRecordMapperTest { @Autowired private ImageRecordMapper mapper; @Test public void testSimply() { ImageRecord record = mapper.selectByPrimaryKey("1"); Assert.assertNotNull(record); Assert.assertEquals("taccisum", record.getBucket()); Assert.assertEquals("image1", record.getKey()); Assert.assertEquals("hash1", record.getHash()); } } 分析原因在github上mapper的issue中有一段作者的回复，说此异常是由于通用方法没有被正确初始化导致。我这边的情况虽然和这个issue不同，但根本原因是一样的。查阅Mybatis-Test的文档可知，@MybatisTest注解不同于@SpringBootTest，它只加载保证Mybatis正常运行所需要的configuration。显然通用Mapper作为Mybatis第三方的扩展，并没有被纳入其中，因而默认情况下通用Mapper的starter是不会被加载的，也就导致通用方法不会被初始化。 The @MybatisTest can be used if you want to test MyBatis components(Mapper interface and SqlSession). By default it will configure MyBatis(MyBatis-Spring) components(SqlSessionFactory and SqlSessionTemplate), configure MyBatis mapper interfaces and configure an in-memory embedded database. MyBatis tests are transactional and rollback at the end of each test by default, for more details refer to the relevant section in the Spring Reference Documentation. Also regular @Component beans will not be loaded into the ApplicationContext.以上是来自mybatis-spring-boot-test-autoconfigure官方文档的描述 解决方案显然，只要让Mapper的starter在MybatisTest的单元测试启动时得以加载即可解决我们的问题。那么如何做呢？Spring Boot提供了一个@ImportAutoConfiguration的注解，因此只需要在单元测试的目标类上使用该注解将MapperAutoConfiguration导入即可 @RunWith(SpringRunner.class) @ImportAutoConfiguration(MapperAutoConfiguration.class) @MybatisTest public class ImageRecordMapperTest { …… } 其中MapperAutoConfiguration是通用Mapper的starter的auto-configure类。]]></content>
      <categories>
        <category>java</category>
        <category>踩坑日记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle学习（五） - 打包发布]]></title>
    <url>%2Flearn_gradle_5.html</url>
    <content type="text"><![CDATA[发布到Maven仓库Maven-Publish插件Gradle将项目发布到Maven仓库需要借助插件，Maven-Publish便是官方推荐的一款插件。Maven-Publish 发布项目通过&#39;maven-publish&#39;引入插件build.gradle apply plugin: 'maven-publish' 发布到本地build.gradle apply plugin: 'java' apply plugin: 'maven-publish' group = 'cn.tac.test' version = '1.0' publishing { publications { mavenJava(MavenPublication) { from components.java } } } publishing { repositories { maven { url "$buildDir/repo" } } } 在以上配置中 publishing{}是由插件为项目创建的扩展（PublishingExtension类型），这个扩展提供了两个block：publications{}（MavenPublication类型）和repositories{}（MavenArtifactRepository 类型），分别用于配置要发布的内容和目标仓库（均可配置多个）。 mavenJava{}被称为发布组件Component，用于配置一项要发布的内容，components.java表示这个组件是通过Java插件添加的java组件，可以简单地理解为就是将当前java项目作为发布内容。 maven{}指定了一个要发布的目标仓库，当前指定了当前项目的build目录下的repo目录，即发布到本地 执行publishing -&gt; publish，可以看到项目成功发布到了build/repo目录中。 tips 两个publishing{}块的内容也可以合在一起写 mavenJava只是一个命名，并无特殊含义，你也可以将其更换为别的名称，但不能与其它发布组件名称重复 mavenJava块中还可以配置要发布的artifact的group、id及version，如果不显式指定，则默认采用当前项目的配置 发布源码在上一步的基础上 新增一个用于获取源码的task 配置发布组件，使其发布的同时额外发布一个包含源码的artifact build.gradle …… //这个task可以获取到源码 task sourceJar(type: Jar) { from sourceSets.main.allJava } publishing { publications { mavenJava(MavenPublication) { …… //配置额外发布的artifact artifact sourceJar { //这个字符串会作为artifact文件的后缀 classifier "sources" } } } } …… 执行task 发布到远程仓库只需要修改url指向远程仓库即可。同时由于远程仓库大多需要认证，因此通常需要通过credentials{}指定用户名和密码 build.gradle …… publishing { repositories { maven { url "http://172.10.10.66:8081/nexus/content/repositories/releases/" credentials { username = "admin" password = "admin123" } } } } …… 条件发布这点相信会点groovy的同学都不会觉得难，用if/else就可以完成。例如下面的配置实现根据version后缀来决定是发布到snapshots还是releases仓库中 build.gralde …… publishing { repositories { def NEXUS_URL = "http://172.10.10.66:8081/nexus/content/repositories/releases/" if (project.version.endsWith("SNAPSHOT")){ NEXUS_URL = "http://172.10.10.66:8081/nexus/content/repositories/snapshots/" } maven { url NEXUS_URL credentials { username = "admin" password = "admin123" } } } } …… 发布多项目父项目的task执行的同时会执行其所有子项目的同一task（最常见的如build任务），因此多项目发布只需要配置好所有子项目的发布配置即可。 build.gradle subprojects { apply plugin: 'java' apply plugin: 'maven-publish' group "cn.tac.test" version "1.0-SNAPSHOT" //因为父项目不需要发布，所以只需要配置子项目的发布配置即可 publishing { publications { mavenJava(MavenPublication) { from components.java } } } publishing { repositories { maven { url "$buildDir/repo" } } } } 执行task，可以看到每个子项目都发布到了其对应的build/repo中。 tips 如果子模块之间互相有依赖，Gradle发布时会自动解析各模块的先后发布顺序，无需我们自己配置 常见问题发布后pom.xml中项目的依赖scope为runtime的问题在发布的时候发现，项目中通过complie依赖的第三方库或其它模块，在发布后由Gradle生成的pom.xml文件中，依赖的scope默认变成了runtime，而非我们期望的compile &lt;?xml version="1.0" encoding="UTF-8"?> …… &lt;groupId>cn.tac.test&lt;/groupId> &lt;artifactId>publishing&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;dependencies> &lt;dependency> &lt;groupId>cn.tac.test&lt;/groupId> &lt;artifactId>module1&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;scope>runtime&lt;/scope> &lt;/dependency> &lt;/dependencies> &lt;/project> 为了达到期望的效果，只需要在发布配置中通过withXML对pom进行修改即可build.gradle publishing { publications { mavenJava(MavenPublication) { from components.java pom.withXml { asNode().dependencies.'*'.findAll() { it.scope.text() == 'runtime' &amp;&amp; project.configurations.compile.allDependencies.find { dep -> dep.name == it.artifactId.text() } }.each() { it.scope*.value = 'compile' } } } } } 再次发布 &lt;?xml version="1.0" encoding="UTF-8"?> …… &lt;groupId>cn.tac.test&lt;/groupId> &lt;artifactId>publishing&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;dependencies> &lt;dependency> &lt;groupId>cn.tac.test&lt;/groupId> &lt;artifactId>module1&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;scope>compile&lt;/scope> &lt;/dependency> &lt;/dependencies> &lt;/project> tips 参考链接 How to change artifactory runtime scope to compile scope? 更多withXML的信息]]></content>
      <categories>
        <category>java</category>
        <category>gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[了解爬虫]]></title>
    <url>%2Fknow_crawler.html</url>
    <content type="text"><![CDATA[什么是爬虫简介爬虫（Web Crawler），也叫网络蜘蛛（Splider），是一种用来自动浏览www网页的程序。爬虫一般从一个URL出发，访问所有关联的URL，并从中提取出感兴趣的内容。爬虫访问网站的过程会消耗目标系统资源，所以有不少网络系统并不默许爬虫工作。因此爬虫要考虑到规划、负载，并且要讲『礼貌』（参考robots.txt）。 网络爬虫 应用场景爬虫主要有以下应用场景 科学研究 如数据挖掘、机器学习、图片处理等 Web安全 如使用爬虫对网站是否存在某一漏洞进行批量验证、利用 产品研发 如采集各个商城物品价格，为用户提供市场最低价 舆情监控 如抓取、分析某社交平台的数据，从而识别出某用户是否为水军 搜索引擎 爬虫是搜索引擎的核心组成部分 爬虫策略一个爬虫的实现主要由四大策略组成 选择策略 指定页面下载的策略，可细分为链接跟随限制，指的是爬虫只搜索特定类型（如HTML）的资源，避免发出过多的请求。或者避免请求一些带有”?”的资源，避免从网站下载无限量的URLURL规范化，指的是以某种一致的方式修改和标准化URL的过程，避免资源的重复爬取路径上移爬取，指爬取每个URL里提示的每个路径，例如对于”http: //www.tac.cn/a/b/c.html&quot;，还会爬取&quot;/a/b&quot;、&quot;/a&quot;和&quot;/&quot;路径。主题爬取，指有条件地进行爬取（例如只爬取与python相关的页面）。 重新访问策略 网站是经常动态变化的，需要估算URL的新鲜度和过时性，以确保是否需要重新访问。 平衡礼貌策略 使用爬虫可能导致一个站点瘫痪，因此爬虫需要遵守一些协议来避免这个问题。 并行策略 并行运行多个进程的爬虫时，为了避免重复下载页面，爬虫系统需要策略来处理爬虫运行时新发现的URL。 简单架构 如图，一个最简单的爬虫架构至少包括爬虫调度端、URL管理器、网页下载器、网页解析器。这几个模块相互作用，从而提取出有价值的数据。其时序图如下 爬虫调度器从URL管理器中获取待爬取的URL，交由下载器进行下载。然后将下载器将下载的页面交由解析器进行解析，解析器又反过来将解析到的URL反馈给URL管理器。如此循环往复，直到没有待抓取的URL，则结束爬取。 ——以上图片来自慕课网Python开发简单爬虫 URL管理器管理待抓取的URL集合和已抓取的URL集合，防止重复抓取、防止循环抓取。 实现方式 内存 关系数据库 NoSQL 网页下载器 urllib2 python官方 * requests 第三方 网页下载分几种场景 直接通过url下载 需要添加请求参数，请求头（伪装浏览器） 需要伪装特殊情景，如cookie、proxy、https、http redirect 页面解析器从网页中提取有价值数据的工具 python的网页解析器 正则表达式 html.parser python官方 beautiful soup4 第三方 * lxml]]></content>
      <categories>
        <category>python</category>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle学习（四） - 自定义任务]]></title>
    <url>%2Flearn_gradle_4.html</url>
    <content type="text"><![CDATA[草稿]]></content>
      <categories>
        <category>java</category>
        <category>gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle学习（三） - 多项目构建]]></title>
    <url>%2Flearn_gradle_3.html</url>
    <content type="text"><![CDATA[前言从第一篇中我们知道了如何构建一个单项目，但仅仅这样是不够的。实现多项目的构建有利于模块化，如此一来我们便能更好地在一个大型项目中分离我们的关注点。 Getting Started创建根项目新建一个文件夹作为根项目目录，执行gradle init $ mkdir multi_project $ gradle init $ ls build.gradle gradle gradlew gradlew.bat settings.gradle 这次我们要关注的重点有两个文件 build.gradle 配置一些应用于所有子项目的公共配置 settings.gradle 描述各项目之间的关系 创建子项目在根目录下分别创建sub-project1和sub-project2目录，然后分别为其创建build.gradle $ mkdir sub-project1 sub-project2 $ touch sub-project1/build.gradle sub-project2/build.gradle 然后在根项目的settings.gradle中添加下面内容以关联子项目 include 'sub-project1' include 'sub-project2' tips 一定要注意子项目只需为其创建build.gradle即可，而非使用gradle init指令初始化，这点很重要 子项目的build.gradle只用于配置该项目特有的一些配置项，公共的配置通过根项目的build.gradle配置 公共配置在根项目的build.gradle中加入以下内容 allprojects{ group = 'cn.tac' version = '0.1' repositories{ maven { url 'http://maven.aliyun.com/nexus/content/groups/public/' } } } subprojects { apply plugin: 'java' sourceCompatibility = 1.8 dependencies { testCompile 'junit:junit:4.12' } } allprojects{} 为所有项目添加配置项，所以group、version、repositories都放在了这个block下 subprojects{} 仅仅为当前项目的子项目添加配置项（不包括当前项目本身），所以根项目不需要的内容（如依赖）放在了这个block下 编写代码分别为子项目创建src目录 $ mkdir -p sub-project1/src/test/java/cn/tac/gradle sub-project2/src/test/java/cn/tac/gradle 并创建单元测试 package cn.tac.gradle; import org.junit.Assert; import org.junit.Test; public class SubProject1Test { @Test public void testSimply() { System.out.println("hello, i'm sub project1"); } } package cn.tac.gradle; import org.junit.Assert; import org.junit.Test; public class SubProject2Test { @Test public void testSimply() { System.out.println("hello, i'm sub project2"); } } 执行构建完成了上述步骤之后，不出意外此时的目录结构应该如下 $ tree . . ├── build.gradle ├── gradle │ └── wrapper │ ├── gradle-wrapper.jar │ └── gradle-wrapper.properties ├── gradlew ├── gradlew.bat ├── settings.gradle ├── sub-project1 │ ├── build.gradle │ └── src │ └── test │ └── java │ └── cn │ └── tac │ └── gradle │ └── SubProject1Test.java └── sub-project2 ├── build.gradle └── src └── test └── java └── cn └── tac └── gradle └── SubProject2Test.java 接下来我们切换到根目录下，执行 $ sh gradlew build 再查看子项目的目录，发现分别多了一个build目录，可见在为根项目执行构建任务时，其include的所有子项目也分别进行了构建。以下分别是两个子项目的build reports 依赖其它项目在dependencies{}中添加依赖即可，例如sub-project2要依赖sub-project1，可以在sub-project2的build.gradle中添加 dependencies { compile project(':sub-project1') }]]></content>
      <categories>
        <category>java</category>
        <category>gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle学习（二） - 常用插件]]></title>
    <url>%2Flearn_gradle_2.html</url>
    <content type="text"><![CDATA[build-scan（构建审视）插件介绍 A build scan is a shareable and centralized record of a build that provides insights into what happened and why. By applying the build scan plugin to your project, you can create a build scan in the Gradle Cloud for free.Creating Build Scans 大概的意思是build scan能为你提供构建过程中发生的what and why信息，在你构建的时候，插件会抓取数据提交到Gradle Cloud，同时返回一个包含构建信息的链接。 工作流程 配置配置方式很简单，只需要在build.gradle中加入 plugins { id 'com.gradle.build-scan' version '1.9' } 添加了插件后，可以通过buildScan块来配置插件，其中有两个license相关的属性是必需要配置的 buildScan { licenseAgreementUrl = 'https://gradle.com/terms-of-service' licenseAgree = 'yes' } import changes（如果没有勾选auto-import），可以看到Gradle Project面板上多了个task tip 如果添加新的plugin，应该确保build-scan总是在第一个位置，否则其之前的插件虽然仍然正常工作，但是无法到抓取相关的构建信息 使用先执行build -&gt; build，再执行这个build scan -&gt; buildScanPublishPrevious，不出意外可以看到terminal中返回了一个链接 9:50:39 PM: Executing external task 'buildScanPublishPrevious'... :buildScanPublishPrevious Publishing build scan... https://gradle.com/s/af72je4qbzhme BUILD SUCCESSFUL Total time: 1.283 secs 9:50:41 PM: External task execution finished 'buildScanPublishPrevious'. 打开链接后可以看到这样一个界面 接下来在任一个单元测试内加入一行让构建失败 Assert.fail(); 再执行一次构建，打开链接查看 可以看到有以下几大优点： 信息展示非常全面丰富、直观 良好的分类、折叠，让用户自己选择展开感兴趣的内容，非常友好 网页形式，非常易于分享（这一点有点类似AngularJS的错误信息，不过没考察是谁借鉴谁的，或许两者都不是原创？）。 相比之下，这里不得不提到一直被人吐槽的Maven的构建信息，真心是非常不友好😒 tip 如果构建时出现”There is no previous build data available to publish.”，可能是没有先执行任一task。 Application插件介绍Application插件可以让你轻松地在本地开发环境下执行JVM应用，同时还可以帮助你将应用打包成一个包含了各类操作系统对应启动脚本的tar and/or zip文件。 The Application Plugin 配置build.gradle apply plugin: 'application' mainClassName = "cn.tac.test.gradle.Application" //指定程序入口类 //applicationDefaultJvmArgs = ["-Dgreeting.language=en"] //应用程序启动时的jvm参数 添加了Application插件后，项目会多出以下几个task run startScripts installDist distZip distTar 具体可以通过tasks任务查看 tips 按照官方的说法，Application插件已经隐式地包括了Java插件和Distribution插件，因此如果你原来引入了这两个插件，现在可以去掉了 使用以我的main函数为例（注意要跟mainClassName属性指定的类一致） package cn.tac.test.gradle; import java.util.Arrays; public class Application { public static void main(String[] args) { if (args.length > 0) { System.out.println("hello, it's you args: " + Arrays.toString(args)); } else { System.out.println("hello, you do not input any args"); } } } 执行应用$ sh gradlew run > Task :run hello, you do not input any args 如果要传入参数，可以配置一下run任务build.gradle run { if(project.hasProperty("myArgs")){ args myArgs } } 上面配置的意思是，如果当前项目的project对象包含有myArgs属性，那么在执行main函数时就将这个属性作为参数传递，之后我们可以这样执行 $ sh gradlew run -PmyArgs="123","abc","qaz" > Task :run hello, it s you args: [123,abc,qaz] 其中-PmyArgs分为两部分 -P，命令行option。作用是指定一个属性的值run，不能省去 myArgs，我们刚刚在run任务中自定义的属性，通过-P指定 打包执行以下脚本可以进行打包 $ sh gradlew distTar distZip 打包好的内容在/build/distributions中，分别多了一个tar文件和一个zip文件，解压后查看目录结构如下 $ tree . . ├── bin │ ├── gradle_cli │ └── gradle_cli.bat └── lib └── gradle_cli-1.0.jar tips 当然你也可以通过build任务来打包，build任务会自动将distTar和distZip任务包括进去]]></content>
      <categories>
        <category>java</category>
        <category>gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle学习（一） - Getting Started]]></title>
    <url>%2Flearn_gradle_1.html</url>
    <content type="text"><![CDATA[前言虽然一直以来都用Maven作为java项目的构建工具，但早就听说过Gradle大名，于是今天终于抽出时间来了解一下这款号称结合了Ant和Maven优点的构建工具。虽然Gradle支持多种语言，但这个系列的文章主要以Java项目构建为主。由于本人不是做Android开发，所以这个系列的文章可能会更偏向于Java Web开发视角。 学习资源推荐官方Documentation 内容非常全面，缺点是全英文（对于英语差的同学简直是噩梦）《跟我学Gradle》 Gralde中文用户组编写的中文系列教程，缺点是还不够完善，有些章节还没有内容《Gradle In Action》中译版 书我没完整看过，只是查资料时读过几个章节，感觉内容还不错 概貌了解同类工具比对AntAnt是第一个“现代”构建工具，于2000年发布基于过程式编程的idea，具备插件功能及通过网络进行依赖管理的功能（结合Apache Ivy）。不足之处是采用XML作为脚本编写格式，不符合过程化编程的初衷。 MavenMaven出现的目的是解决Ant带来的一些问题，发布于2004年。Maven依靠约定并提供现成的可调用的目标，首创了从网络下载依赖的功能。依然采用XML作为配置文件（因此同样有跟Ant一样难以定制化构建过程的缺点）。另外Maven虽然聚焦于依赖管理，但并不能很好地处理相同库文件不同版本之间的冲突（不如Ivy）。 GradleGradle结合了两者的优点，并在此基础上做了许多改进。Gradle使用基于Groovy的DSL编写构建脚本，可以更细致地控制编译打包过程（这也是为什么Android Studio默认采用Gradle作为构建工具的原因）。Gradle对多模块项目有很好的支持。Gradle支持多语言，包括java、groovy、scala、c++等。Gradle使用Apache Ivy处理依赖，因此依赖管理方面优于Maven。同时Gradle可以使用多种类型的远程仓库，如Maven仓库、Ivy仓库。 关于DSLDSL是Domain Specific Language的缩写，即领域特定语言。同字面上的意思，就是专用于处理某一领域问题的特定语言，例如用于web页面开发的HTML语言、用于GNU Emacs的Emacs Lisp等，甚至有一些简单的DSL只用于某个单应用程序（也称为Mini-Languages）。 由此可见，Gradle使用的DSL应该是一种专用于项目构建的语言。 更多内容 Getting Started - IntelliJ IDEA初次接触为了能够快速看到效果，所以直接使用ide来入门。不过为了对Gradle有更深入的了解，往后的练习项目将全部使用命令行构建。 创建项目 New -&gt; Project -&gt; Gradle，新建一个Gradle项目（就像Maven一样，IDEA内置了Gradle，所以不需要我们手动去安装了） 填写GroupId、ArtifactId、Version（这些跟Maven是一样的） 这里勾选上Create directories for empty content roots automatically选项，让IDEA帮我们创建好目录结构 Finish，初次构建可能会花费较长的时间（跟Maven一样，要从网络下载一些东西，比如项目模板），构建好后的目录结构如下 来看下各folder&amp;file的含义： .gradle Gradle相关的支持文件，一般不用关心 gradle wrapper The wrapper is a small script and supporting jar and properties file that allows a user to execute Gradle tasks even if they don’t already have Gradle installed. Generating a wrapper also ensures that the user will use the same version of Gradle as the person who created the project.Creating New Gradle Project 大意为，wrapper里面是一些简单的脚本、使用户能在没有安装Gradle的情况下也能执行Gradle任务的supporting jar及properties文件等，同时wrapper还能确保用户执行Gradle任务时使用的版本与项目创建者使用的Gradle版本相同。总之是一个开发人员基本不需要关心的目录。 src 源码目录，采用了与Maven相同的结构 build.gradle Gradle的构建配置文件（build file），需要我们编写内容（类似Maven的pom.xml）。按照官方的描述，每个build.gradle都配置了一个org.gradle.api.Project类的实例，并且这个实例会有许多内建的方法和属性（稍后CLI项目中可以看到gradlew properties列出了一堆project的属性）。build.gradle的DSL参考 gradlew/gradlew.bat 分别用于类unix系统和windows系统下的wrapper脚本，之后可以看到，创建了wrapper后我们所有的指令都通过wrapper脚本来执行 settings.gradle 与多模块项目配置有关的文件，用于描述项目模块之间的关系 Hello World创建好项目后可以看到，已经有许多配置好的东西了，如junit依赖、Gradle wrapper等，所以现在已我们直接可以直接写单元测试 public class GettingStarted { @Test public void testSimply() { System.out.println("hello gradle"); } } 执行可以看到控制台输出 hello gradle Getting Started - CLI使用ide写个Gradle的Hello World确实非常简单，但使用CLI来搭建项目，能让我们对Gradle有更加深入的了解。接下来我们将尝试用CLI写一个Hello World。 安装Gradle由于我们这次用的是CLI，所以必须手动安装Gradle。以我用的MacOS为例，打开terminal，run $ brew install gradle 安装前必须确保安装了jdk1.7以上版本（我用的Gradle 4.1版本的要求），其它系统用户可以参考Installation 然后run，若已成功安装可以看到 $ gradle --version ------------------------------------------------------------ Gradle 4.1 ------------------------------------------------------------ Build time: 2017-08-07 14:38:48 UTC Revision: 941559e020f6c357ebb08d5c67acdb858a3defc2 Groovy: 2.4.11 Ant: Apache Ant(TM) version 1.9.6 compiled on June 29 2015 JVM: 1.8.0_121 (Oracle Corporation 25.121-b13) OS: Mac OS X 10.12.4 x86_64 创建工程创建一个空文件夹作为工程目录，同时创建一个build.gradle空文件 $ mkdir gradle_cli $ cd gradle_cli $ touch build.gradle 然后执行以下命令生成Gradle Wrapper $ gradle wrapper 可以看到当前目录的变化 $ tree . . ├── build.gradle ├── gradle │ └── wrapper │ ├── gradle-wrapper.jar │ └── gradle-wrapper.properties ├── gradlew └── gradlew.bat 现在可以通过wrapper脚本来执行各项任务，这样可以确保在更换了环境后依然能使用创建项目时使用的Gradle版本进行构建。 查看properties信息（以下输出做了大量删减） $ sh gradlew properties > Task :properties ------------------------------------------------------------ Root project ------------------------------------------------------------ allprojects: [root project 'gradle_cli'] ant: org.gradle.api.internal.project.DefaultAntBuilder@70e298ee …… buildDir: /Users/tac/Documents/studyspace/src/java/gradle_cli/build buildFile: /Users/tac/Documents/studyspace/src/java/gradle_cli/build.gradle buildScriptSource: org.gradle.groovy.scripts.UriScriptSource@460e8e7a …… depth: 0 description: null displayName: root project 'gradle_cli' …… gradle: build 'gradle_cli' group: …… plugins: [org.gradle.api.plugins.HelpTasksPlugin@23fb712b] …… project: root project 'gradle_cli' …… projectDir: /Users/tac/Documents/studyspace/src/java/gradle_cli …… repositories: repository container resources: org.gradle.api.internal.resources.DefaultResourceHandler@7cad9556 rootDir: /Users/tac/Documents/studyspace/src/java/gradle_cli rootProject: root project 'gradle_cli' …… state: project state 'EXECUTED' status: release subprojects: [] tasks: task set version: unspecified 可以看到其中大多数属性都已经有了默认的值，这也恰好验证了Gradle约定优于配置的原则。如果我们需要修改一些属性值，可以通过写build.gradle文件来进行配置 description = 'A Gradle build project for CLI' version = '1.0' group = 'cn.tac.test' 再次查看properties可以看到属性已经更改了 $ sh gradlew properties | grep -E "group|description|version" description: A Gradle build project for CLI group: cn.tac.test version: 1.0 tip 你也可以先生成Wrapper再创建build.gradle，并不会有影响 除了手动创建之外，还可以通过gradle init指令来初始化项目。初始化的内容包括执行gradle wrapper，以及自动生成build.gradle和settings.gradle，并且生成的文件里面已经有了一些自动生成的配置（默认是注释状态，即未启用）。 配置环境由于我们是手动创建的空build.gradle，要构建java项目，我们还需要做一些简单的配置。在build.gradle加入目标工程语言（上面提过，Gradle是支持多语言的）及版本、依赖及下载依赖的仓库的配置 apply plugin: 'java' sourceCompatibility = 1.8 repositories { mavenCentral() } dependencies { testCompile group: 'junit', name: 'junit', version: '4.12' } tips 可以通过dependencies任务查看当前项目的依赖信息。 Hello World配置好环境后，我们创建源码目录及单元测试类 $ mkdir -p src/main/java src/main/resources src/test/java src/test/resources $ cd src/test/java $ mkdir -p cn/tac/test $ cd cn/tac/test $ touch HelloWorld.java 然后执行构建，如果不知道有哪些tasks可以执行，可以通过以下命令来查看 $ sh gradlew tasks > Task :tasks ------------------------------------------------------------ All tasks runnable from root project - A Gradle build project for CLI ------------------------------------------------------------ Build tasks ----------- assemble - Assembles the outputs of this project. build - Assembles and tests this project. …… Build Setup tasks ----------------- init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Documentation tasks ------------------- javadoc - Generates Javadoc API documentation for the main source code. Help tasks ---------- …… Verification tasks ------------------ check - Runs all checks. test - Runs the unit tests. Rules ----- …… $ sh gradlew build 完了可以看见项目根目录下多了一个build目录，里面的内容就是执行构建的产物。与Maven不同的是，有个reports目录是Gradle生成的HTML格式的构建报告，可以通过浏览器打开查看 tip 有没有发现sh gradlew tasks出来的列表有点像IDEA Gradle Project面板上的tasks节点？😃 在项目根目录下使用gradle跟gradlew执行task的效果基本是一样的，区别在于gradle会使用本地安装的Gradle版本进行构建，而gradlew会使用创建项目时使用的gradle版本进行构建，如果本地没有搜索到这个版本，则会自动下载]]></content>
      <categories>
        <category>java</category>
        <category>gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[解释器模式实践]]></title>
    <url>%2Finterpreter_pattern_in_action.html</url>
    <content type="text"><![CDATA[源码地址Github 关于解释器模式解释器模式是行为型模式的一种，关于解释器模式的定义从网上摘抄了一段 给定一种语言，并定义代表其文法的相应解释器，并用该解释器来解析使用该语言的编写的内容JAVA Design pattern 其中的关键字有，语言文法、解释器。 语言文法，如：下面要提到的四则运算表达式、我们日常使用的各种编程语言、类英语语法的SQL语言、一些框架里面的特定语言（如Spring的SpEL）等等，都是具有固定文法的语言，这意味着这些语言均可以用解释器模式来解析。当然，除此之外你也可以根据你的需求自己创造一种语言。 解释器，用于让计算机『认识』这些语言的工具，需要由我们程序员编码实现。越复杂的语言文法，其相应的解释器实现难度越大，比如人类的『自然语言』。 UML图 JAVA Design pattern 其实并不算复杂（因为解释器模式最复杂的部分在于将语言解析为对象的过程，稍后的实践中可以看到），跟组合模式的UML图有点相像（同为树型数据结构），下面是两个比较重要的类： NumberExpression: 数值表达式（也称终结点表达式，意为表达式解析的终结点，可以简单理解为树型结构的叶子节点），执行interpret()一般会得到一个常量供复合表达式计算 MultiplyExpression: 复合表达式（也称非终结点表达式，可以简单理解为树型结构的枝节点），一般包括一个或多个Expression类型的子节点，子节点数即代表该复合表达式的操作数数量，执行interpret()一般会得到其子节点互相作用后的结果 编码实践为了节省篇幅，这里只贴出部分重要代码，完整代码可从开头的源码地址下载。 简单四则运算单元测试类cn.tac.test.interpreter.arithmetic.simple.SimpleCalculatorTest 实现效果支持最简单的四则运算，不支持括弧运算，不支持优先级运算符。因为语言文法较简单，所以解析器较易实现，适合作为Hello World。 相关类Node: 即Expression public interface Node { int interpret(); } ValueNode: 即NumberExpression，代表操作数 public abstract class ValueNode implements Node{ protected int value; public ValueNode(int value) { this.value = value; } } SymbolNode: 即MultiplyExpression，代表运算符（如+-*/），因为均为二元操作符，所以只有两个子节点 public abstract class SymbolNode implements Node { protected Node left; protected Node right; public SymbolNode(Node left, Node right) { this.left = left; this.right = right; } } SimpleValueNode: 简单地返回数值本身 public class SimpleValueNode extends ValueNode { public SimpleValueNode(int value) { super(value); } public int interpret() { return value; } } 运算符节点: 代表+-*/ public class PlusNode extends SymbolNode { public PlusNode(Node left, Node right) { super(left, right); } public int interpret() { return left.interpret() + right.interpret(); } } public class MinusNode extend SymbolNode{……} public class MulNode extend SymbolNode{……} public class DivNode extend SymbolNode{……} Parser: public class Parser { public static final String SPLITTER = " "; //为了方便解析，用空格将表达式分隔开 public Node parse(String statement) { String[] arr = statement.split(SPLITTER); LinkedList&lt;Node> nodeStack = new LinkedList&lt;>(); boolean symbolFlag = false; String symbol = ""; for (String item : arr) { if (isSymbolNode(item)) { symbol = item; symbolFlag = true; } else if (isValueNode(item)) { nodeStack.push(new SimpleValueNode(Integer.parseInt(item))); if (symbolFlag) { Node right = nodeStack.pop(); //后入先出 Node left = nodeStack.pop(); nodeStack.push(mapSymbolNode(left, right, symbol)); symbolFlag = false; } } else { throw new RuntimeException("表达式格式有误"); } } return nodeStack.pop(); } …… } Calculator: public class SimpleCalculator { public int calculate(String statement){ Parser parser = new Parser(); return parser.parse(statement).interpret(); } } Parser解析过程 将表达式分割为数组，并遍历 从数组中获取一个元素，若为数值，则直接转换为节点并入栈；若为运算符，则只进行标记，等待下一个节点入栈后，将其与上一个节点一同取出，并合并为一个复合表达式节点入栈； 反复执行步骤2，直到数组末尾 将栈中的元素pop并返回（如果表达式无误，最终栈中应该只有一个元素，否则应该在解析过程中抛出异常） 完整四则运算单元测试类cn.tac.test.interpreter.arithmetic.full.FullCalculatorTest cn.tac.test.interpreter.arithmetic.full.ParserTest 实现效果支持完整的四则运算，包括括弧运算，优先级运算。由于相较于上一个实践文法上复杂了许多，不能用同样的方式直接解析，而是应该先转换为后缀表达式再进行解析。 后缀表达式// todo:: 相关类节点值节点和运算符节点类仍采用和上个实践相同的类。 Parser public class Parser { public static final String SPLITTER = " "; public Node parse(String statement) { LinkedList&lt;Node> stack = new LinkedList&lt;>(); String[] segments = convert2Postfix(statement).split(SPLITTER); for (String segment : segments) { if (isValueNode(segment)) { stack.push(new SimpleValueNode(Integer.parseInt(segment))); } else if (isOperator(segment)) { Node rightNode = stack.pop(); Node leftNode = stack.pop(); stack.push(doOperate(leftNode, rightNode, segment)); } else { throw new RuntimeException(); } } return stack.pop(); } public String convert2Postfix(String statement) { LinkedList&lt;String> symbolStack = new LinkedList&lt;>(); //临时存放符号节点的栈 LinkedList&lt;String> stack = new LinkedList&lt;>(); String[] nodes = statement.split(SPLITTER); for (String node : nodes) { if (isSymbolNode(node)) { if (symbolStack.size() == 0 || "(".equals(node)) { symbolStack.push(node); } else if (isOperator(node)) { String topSymbol = symbolStack.peek(); if (priority(node) &lt;= priority(topSymbol)) { while (symbolStack.peek() != null) { if (priority(symbolStack.peek()) &lt; priority(node)) { break; } else { stack.push(symbolStack.pop()); } } } symbolStack.push(node); } else if (")".equals(node)) { while (!"(".equals(symbolStack.peek())) { stack.push(symbolStack.pop()); } symbolStack.pop(); } else { throw new RuntimeException("不支持的运算符"); } } else if (isValueNode(node)) { stack.push(node); } else { throw new RuntimeException("不支持的运算符"); } } //到数组尾部后，还有一些符号在栈内 while (symbolStack.peek() != null) { stack.push(symbolStack.pop()); } StringBuilder sb = new StringBuilder(); String tmp; while (isNull(tmp = stack.pollLast())) { sb.append(tmp); sb.append(SPLITTER); } return sb.toString().trim(); } …… } Calculator public class FullCalculator { public int calculate(String statement){ Parser parser = new Parser(); return parser.parse(statement).interpret(); } } Parser解析过程主要分为两大步骤一、将中缀表达式转换为后缀表达式 将表达式分割为数组，并遍历 从数组中获取一个元素 若为数值，则直接入存放结果的栈，以下简称结果栈 若为符号，则判断 若符号栈目前为空，或该符号为左括弧”(“，则直接入临时存放符号的栈，以下简称符号栈 若当前符号为操作符（+-*/，不包括括弧），则从符号栈peek（不是pop）一个符号，比如两者优先级，若 当前符号优先级&lt;=从栈顶取出的符号优先级，则从符号栈pop一个符号并push至结果栈，直至 符号栈为空 or 栈顶符号优先级&lt;=当前符号优先级，然后将当前符号入符号栈。这里需要注意的是，左右括弧也有优先级，且优先级应小于操作符，否则在这里需要单独处理pop时遇到左括弧的问题 若为右括弧”)”，则将符号栈pop符号并push至结果栈，直到遇到左括弧（遇到后pop并丢弃） 反复执行步骤2，直到数组末尾 到数组尾部后，多数情况都会有一些符号还留在符号栈内，应将其取出并push至结果栈 将结果栈构建为字符串，并返回 二、将后缀解析为java对象 将表达式分割为数组，并遍历 从数组中获取一个元素，若为数值，则直接转换成数值节点并入栈；若为操作符（转换成后缀表达式后已经没有括弧了），则从栈顶取出两个节点，并合并为一个复合表达式节点并入栈 反复执行步骤2，直到数组末尾 将栈中的元素pop并返回（如果表达式无误，最终栈中应该只有一个元素，否则应该在解析过程中抛出异常） 当然，你也可以将这两个步骤合二为一，在转换的同时将其转换为java对象，但非实际应用中个人还是建议将两个步骤分离，原因有二： 降低编码的复杂度 方便进行单元测试（尤其是将后缀表达式转换为中缀表达式的步骤，比较容易出错）]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于java]]></title>
    <url>%2Fabout_java.html</url>
    <content type="text"><![CDATA[java体系 java不仅仅是一门编程语言，还是由一系列计算机软件和规范形成的技术体系。这个体系提供了完整的用于软件开发和跨平台部署的支持环境。 Sun官方定义的java技术体系包括以下内容： java程序设计语言 各硬件平台上的java虚拟机（jvm） Class文件格式 java api类库 来自商业机构和开源社区的第三方类库 以上1、2、4点统称为jdk，jdk是支持java程序开发的最小环境.java api中的java se api子集和java虚拟机统称为jre，jre是java程序运行的最小环境. java除了一门优秀的编程语言外，还有许多不可忽视的优点： 摆脱硬件平台束缚，实现跨平台 内存管理，避免绝大部分内存泄露和指针越界问题 热点代码检测和运行时编译及优化 完善的应用程序编程接口及丰富、优秀的第三方类库 java技术体系按照所服务的领域来划分，可以分为四大模块：java card java se java me java ee jdk发展史java语言前身：oak 1995年，oak更名为java，同时sun正式提出”write once, run anywhere”的口号，标识着java正式诞生 1996年，jdk1.0发布，并提供了一个纯解释性的java虚拟机（Sun Classic VM） 1997年，jdk1.l发布，代表技术有：jdbc, rmi, java beans, .jar文件等 1998年，jdk1.2发布，正式把java技术体系划分为3个方向：java se, java me, java ee，代表技术较多，如ejb, java pluin-in, swing等。java虚拟机中第一次内置了jit编译器，且附带了HotSpot作为备选 2000年，jdk1.3发布，将HotSpot作为默认虚拟机，并提供jndi作为平台级服务，从此后Sun公司基本保持两年一个主版本的更新速度 2002年，jdk1.4发布，标志着java正式走向成熟，代表技术有：正则表达式、异常链、nio、日志类、xml解析器等等 2004年，jdk1.5(jdk5)发布，主要是对java语言的易用性方面做了改进 2006年，jdk1.6(jdk6)发布，除了改进之外，Sun正式将java开源（GPL协议），并建立了OpenJDK组织对其进行管理jdk1.7(jdk7)开发，Sun公司由于各种原因，最终无法按计划完成。2009年Oracle公司将其收购后，对jdk1.7的内容进行大幅度裁剪，并把剩余内容延迟到jdk1.8中。jdk1.7于2011年正式发布 2013年，jdk1.8(jdk8)发布，加入了lambda表达式及函数式接口等支持 部分jvm介绍 Sun Classic VMr Classic VM是Sun公司发布的第一款商用java虚拟机，作为默认虚拟机与jdk1.0绑定发布，技术较为原始 在jdk1.2之前都是唯一存在的虚拟机 只能使用纯解释器方式来运行java程序 如果要使用JIT编译器，必须外挂 Exact VM 为了解决Classic VM存在的各种问题而开发，在jdk1.2期间发布 具备了现代高性能虚拟机的原型 在商用只存在了短暂的时间后，即被更为优秀的HotSpot取代 Sun HotSpot VM 是SunJDK及OpenJDK中所带的虚拟机，也是目前使用最广的虚拟机 最初是由一家名为”Longview Teconologies”的小公司设计，在1997年该公司被Sun收购 其即有Sun之前两款虚拟机的优点，也有许多自身的技术优势如热点代码探测能力 Sun Mobile-Embedded VM/Meta-Circular VM Sun公司针对移动和嵌入式市场的虚拟机 BEA JRockit 一款专门为服务端硬件和服务端应用场景而专门优化的虚拟机 不关注应用程序的启动速度，因而其内部不含解析器的实现，全部代码靠JIT编译运行 除此之外，其垃圾收集器和MissionControl等服务套件的实现也一直在众多虚拟机中保持领先的水平 IBM J9 VM Azul VM/BEA Liquid VM Apache Harmony/Google Android Dalvik VM Microsoft JVM]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
  </entry>
</search>
